{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from grader import score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# NLP: Analyzing Review Text\n",
    "\n",
    "Unstructured data makes up the vast majority of data.  This is a basic intro to handling unstructured data.  Our objective is to be able to extract the sentiment (positive or negative) from review text.  We will do this from Yelp review data.\n",
    "\n",
    "The first three questions task you to build models, of increasing complexity, to predict the rating of a review from its text.  These models will be assessed based on the root mean squared error of the number of stars predicted.  There is a reference solution (which should not be too hard to beat) that defines the score of 1.\n",
    "\n",
    "The final question asks only for the result of a calculation, and your results will be compared directly to those of a reference solution.\n",
    "\n",
    "## A note on scoring\n",
    "It **is** possible to score >1 on these questions. This indicates that you've beaten our reference model - we compare our model's score on a test set to your score on a test set. See how high you can go!\n",
    "\n",
    "## Download and parse the data\n",
    "\n",
    "To start, let's download the dataset from Amazon S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!aws s3 sync s3://dataincubator-course/mldata/ . --exclude '*' --include 'yelp_train_academic_dataset_review.json.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The training data are a series of JSON objects, in a gzipped file. Python supports gzipped files natively: [gzip.open](https://docs.python.org/2/library/gzip.html) has the same interface as `open`, but handles `.gz` files automatically.\n",
    "\n",
    "The built-in json package has a `loads()` function that converts a JSON string into a Python dictionary.  We could call that once for each row of the file. [ujson](http://docs.micropython.org/en/latest/library/ujson.html) has the same interface as the built-in `json` library, but is *substantially* faster (at the cost of non-robust handling of malformed json).  We will use that inside a list comprehension to get a list of dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import ujson as json\n",
    "\n",
    "with gzip.open('yelp_train_academic_dataset_review.json.gz') as f:\n",
    "    data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import ujson as json\n",
    "import random\n",
    "\n",
    "#Fraction to subsample, needs to be between 0 and 1\n",
    "subsample = 0.1\n",
    "\n",
    "with gzip.open('yelp_train_academic_dataset_review.json.gz') as f:\n",
    "    data = [json.loads(line) for line in f if random.random() < subsample]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Scikit Learn will want the labels in a separate data structure, so let's pull those out now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stars = [row['stars'] for row in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1012913\n",
      "[{u'votes': {u'funny': 0, u'useful': 2, u'cool': 1}, u'user_id': u'Xqd0DzHaiyRqVH3WRG7hzg', u'review_id': u'15SdjuK7DmYqUAj6rjGowg', u'text': u\"dr. goldberg offers everything i look for in a general practitioner.  he's nice and easy to talk to without being patronizing; he's always on time in seeing his patients; he's affiliated with a top-notch hospital (nyu) which my parents have explained to me is very important in case something happens and you need surgery; and you can get referrals to see specialists without having to see him first.  really, what more do you need?  i'm sitting here trying to think of any complaints i have about him, but i'm really drawing a blank.\", u'business_id': u'vcNAWiLM4dR7D2nwwJ7nCA', u'stars': 5, u'date': u'2007-05-17', u'type': u'review'}]\n",
      "dr. goldberg offers everything i look for in a general practitioner.  he's nice and easy to talk to without being patronizing; he's always on time in seeing his patients; he's affiliated with a top-notch hospital (nyu) which my parents have explained to me is very important in case something happens and you need surgery; and you can get referrals to see specialists without having to see him first.  really, what more do you need?  i'm sitting here trying to think of any complaints i have about him, but i'm really drawing a blank.\n"
     ]
    }
   ],
   "source": [
    "print len(stars)\n",
    "print data[:1]\n",
    "print data[0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Notes:\n",
    "1. [Pandas](http://pandas.pydata.org/) is able to read JSON text directly.  Use the `read_json()` function with the `lines=True` keyword argument.  While the rest of this notebook will assume you are using a list of dictionaries, you can complete it with dataframes, if you so desire. Some of the example code will need to be modified in this case.\n",
    "\n",
    "2. There are obvious miscodings in the data.  There is no need to try to correct them.\n",
    "\n",
    "## Building models\n",
    "\n",
    "For the first three questions, you will need to build and train an estimator to predict the star rating from the text of a review.  We recommend building a pipeline out of transformers and estimators provided by Scikit Learn.  You can decide whether these pipelines should take full review objects or just their text as input to the `fit()` and `predict()` methods, but it does pay to be consistent.\n",
    "\n",
    "You may find it useful to serialize the trained models to disk.  This will allow you to reload the models after restarting the notebook, without needing to retrain them.  We recommend using the [`dill` library](https://pypi.python.org/pypi/dill) for this (although the [`joblib` library](http://scikit-learn.org/stable/modules/model_persistence.html) also works).  Use\n",
    "```python\n",
    "dill.dump(estimator, open('estimator.dill', 'w'))\n",
    "```\n",
    "to serialize the object `estimator` to the file `estimator.dill`.  If you have trouble with this, try setting the `recurse=True` keyword args in the call of `dill.dump()`.  The estimator can be deserialized with\n",
    "```python\n",
    "estimator = dill.load(open('estimator.dill', 'r'))\n",
    "```\n",
    "\n",
    "You may run into trouble with the size of your models and Digital Ocean's memory limit. This is a major concern in real-world applications. Your production environment will likely not be that different from Digital Ocean and being able to deploy there is important. Think about what information the different stages of\n",
    "your pipeline need and how you can reduce the memory footprint.\n",
    "\n",
    "Additionally, you may notice that your serialized models are very large and take a long time to load.  Some hints to reduce their size:\n",
    "\n",
    "- If you are using `GridSearchCV` to find the optimal values of hyperparameters (and you should be), the resultant object will contain many copies of the estimator that aren't needed any more.  Instead of serializing the whole `GridSearchCV`, serialize just the estimator with the correct hyperparameters.  This can be accessed through the `.best_estimator_` attribute of the `GridSearchCV` object.  Alternatively, the `.best_params_` attribute gives the best values of the hyperparameters.\n",
    "\n",
    "- The `CountVectorizer` keeps track of all words that were excluded from vectorization in its `.stop_words_` attribute.  This can be interesting to examine, but isn't needed for predictions.  Set this attribute to the empty list before serializing it to save disk space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import dill\n",
    "import numpy as np\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn import base\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn import datasets, linear_model, utils, preprocessing\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, TransformerMixin\n",
    "from sklearn import neighbors\n",
    "from sklearn import ensemble\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Questions\n",
    "\n",
    "Each of the \"model\" questions asks you to create a function that models the number of stars given in a review from the review text.  It will be passed a list of dictionaries.  Each of these will have the same format as the JSON objects you've just read in.  This function should return a list of numbers of the same length, giving the predicted star ratings.\n",
    "\n",
    "This function is passed to the `score()` function, which will receive input from the grader, run your function with that input, report the results back to the grader, and print out the score the grader returned.  Depending on how you constructed your estimator, you may be able to pass the predict method directly to the `score()` function.  If not, you will need to write a small wrapper function to mediate the data types.\n",
    "\n",
    "## 1. bag_of_words_model\n",
    "Build a linear model predicting the star rating based on the count of the words in each document (bag-of-words model).  Use a [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) or [HashingVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer) to produce a feature matrix giving the counts of each word in each review.  Feed this in to linear model, such as `Ridge` or `SGDRegressor`, to predict the number of stars from each review.\n",
    "\n",
    "**Hints**:\n",
    "1. Don't forget to use tokenization!  This is important for good performance but it is also the most expensive step.  Try vectorizing as a first initial step and then running grid-serach and cross-validation only on of this pre-processed data.  `CountVectorizer` has to memorize the mapping between words and the index to which it is assigned.  This is linear in the size of the vocabulary.  The `HashingVectorizer` does not have to remember this mapping and will lead to much smaller models.\n",
    "\n",
    " ```python\n",
    " from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    " text = [row['text'] for row in data]\n",
    " X = CountVectorizer().fit_transform(text)\n",
    "\n",
    " # Now, this can be run with many different parameters\n",
    " # without needing to retrain the vectorizer:\n",
    " model.fit(X, stars, hyperparameter=something)\n",
    " ```\n",
    "\n",
    "2. Try choosing different values for `min_df` (minimum document frequency cutoff) and `max_df` in `CountVectorizer`.  Setting `min_df` to zero admits rare words which might only appear once in the entire corpus.  This is both prone to overfitting and makes your data unmanageably large.  Don't forget to use cross-validation or to select the right value.  Notice that `HashingVectorizer` doesn't support `min_df`  and `max_df`.  However, it's not hard to roll your own transformer that solves for these.\n",
    "\n",
    "3. Try using [LinearRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression) or [RidgeCV](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html#sklearn.linear_model.RidgeCV).  If the memory footprint is too big, try switching to [Stochastic Gradient Descent](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor) You might find that even ordinary linear regression fails due to the data size.  Don't forget to use [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) to determine the regularization parameter!  How do the regularization parameter `alpha` and the values of `min_df` and `max_df` from `CountVectorizer` change the answer?\n",
    "\n",
    "4. You will likely pick up several hyperparameters between the tokenization step and the regularization of the estimator.  While is is more strictly correct to do a grid search over all of them at once, this can take a long time. Quite often, doing a grid search over a single hyperparameter at a time can produce similar results.  Alternatively, the grid search may be done over a smaller subset of the data, as long as it is representative of the whole.\n",
    "\n",
    "5. Finally, assemble a pipeline that will transform the data from records all the way to predictions.  This will allow you to submit its predict method to the grader for scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class ColumnSelectTransformer(base.BaseEstimator, base.TransformerMixin):\n",
    "    # modified only one column allowed\n",
    "    def __init__(self, col_name):\n",
    "        self.col_name = col_name  # We will need these in transform()\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # This transformer doesn't need to learn anything about the data,\n",
    "        # so it can just return self without any further processing\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Return an array with the same number of rows as X and one\n",
    "        # column for each in self.col_names\n",
    "        data = []\n",
    "        col = self.col_name\n",
    "        for row in X:\n",
    "            new_row = row[col]\n",
    "            data.append(new_row)\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#text = [row['text'] for row in data]\n",
    "text = [ data[0]['text'], data[1]['text']]\n",
    "print text\n",
    "\n",
    "X = CountVectorizer().fit_transform(text)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "mydata = data[:20000]\n",
    "mystars = stars[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "bag_of_words_est = Pipeline([\n",
    "    ('trans', ColumnSelectTransformer('text')),# Column selector (remember the ML project?)\n",
    "    ('cvec', CountVectorizer(min_df=1, max_df=1.0, ngram_range=(1,2))),# Vectorizer\n",
    "    # Frequency filter (if necessary)\n",
    "    ('ridgereg', Ridge(alpha=95))# Regressor\n",
    "])\n",
    "\n",
    "bag_of_words_est.fit(mydata, mystars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "frang = [90+p/10.0 for p in range(0, 10)]\n",
    "print frang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "frang = [90+p/10.0 for p in range(0, 10)]\n",
    "\n",
    "param_grid = {'cvec__min_df': xrange(1,2,1) ,\n",
    "              'ridgereg__alpha': xrange(900,1200,100),\n",
    "              #'cvec__max_df': [0.8,0.9,0.95,1.0] ,\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(bag_of_words_est, param_grid=param_grid, verbose=10)\n",
    "grid_search.fit(mydata, mystars) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(grid_search.best_estimator_.get_params())\n",
    "best_q1 = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score('nlp__bag_of_words_model', best_q1.predict)  #bag_of_words_est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2. normalized_model\n",
    "Normalization is key for good linear regression. Previously, we used the count as the normalization scheme.  Add in a normalization transformer to your pipeline to improve the score.  Try some of these:\n",
    "\n",
    "1. You can use the \"does this word present in this document\" as a normalization scheme, which means the values are always 1 or 0.  So we give no additional weight to the presence of the word multiple times.\n",
    "\n",
    "2. Try using the log of the number of counts (or more precisely, $log(x+1)$). This is often used because we want the repeated presence of a word to count for more but not have that effect tapper off.\n",
    "\n",
    "3. [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) is a common normalization scheme used in text processing.  Use the [TFIDFTransformer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer). There are options for using `idf` and taking the logarithm of `tf`.  Do these significantly affect the result?\n",
    "\n",
    "Finally, if you can't decide which one is better, don't forget that you can combine models with a linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "normalized_est = Pipeline([\n",
    "    ('trans', ColumnSelectTransformer('text')),# Column selector (remember the ML project?)\n",
    "    ('cvec', CountVectorizer(min_df=1, max_df=1.0, ngram_range=(1,2))),# Vectorizer\n",
    "    ('tfvec', TfidfTransformer()),# Vectorizer\n",
    "    # Frequency filter (if necessary)\n",
    "    ('ridgereg', Ridge(alpha=1))# Regressor\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............... ridgereg__alpha=10, score=0.371539, total=  12.1s\n",
      "[CV] ridgereg__alpha=10 ..............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:  2.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............... ridgereg__alpha=10, score=0.403140, total=  12.7s\n",
      "[CV] ridgereg__alpha=14 ..............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:  2.7min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............... ridgereg__alpha=14, score=0.340917, total=  12.2s\n",
      "[CV] ridgereg__alpha=14 ..............................................\n",
      "[CV] ............... ridgereg__alpha=14, score=0.330828, total=  12.3s\n",
      "[CV] ridgereg__alpha=14 ..............................................\n",
      "[CV] ............... ridgereg__alpha=14, score=0.360037, total=  11.8s\n",
      "[CV] ridgereg__alpha=18 ..............................................\n",
      "[CV] ............... ridgereg__alpha=18, score=0.304644, total=  11.8s\n",
      "[CV] ridgereg__alpha=18 ..............................................\n",
      "[CV] ............... ridgereg__alpha=18, score=0.298191, total=  12.1s\n",
      "[CV] ridgereg__alpha=18 ..............................................\n",
      "[CV] ............... ridgereg__alpha=18, score=0.326207, total=  12.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:  4.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('trans', ColumnSelectTransformer(col_name='text')), ('cvec', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(...it_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'ridgereg__alpha': xrange(2, 22, 4)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=10)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'ridgereg__alpha': xrange(2,20,4),\n",
    "              #'cvec__min_df': xrange(1,2,1) , \n",
    "              #'cvec__max_df': [0.8,0.9,0.95,1.0] ,\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV( normalized_est, param_grid=param_grid, verbose=10)\n",
    "grid_search.fit(mydata, mystars) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cvec__analyzer': u'word', 'cvec__tokenizer': None, 'cvec__vocabulary': None, 'cvec__decode_error': u'strict', 'cvec': CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'ridgereg__max_iter': None, 'tfvec__smooth_idf': True, 'ridgereg__fit_intercept': True, 'ridgereg__alpha': 2, 'ridgereg__tol': 0.001, 'cvec__lowercase': True, 'cvec__binary': False, 'tfvec__sublinear_tf': False, 'cvec__min_df': 1, 'cvec__encoding': u'utf-8', 'cvec__ngram_range': (1, 2), 'ridgereg__random_state': None, 'cvec__dtype': <type 'numpy.int64'>, 'cvec__token_pattern': u'(?u)\\\\b\\\\w\\\\w+\\\\b', 'tfvec__norm': u'l2', 'cvec__input': u'content', 'cvec__max_df': 1.0, 'cvec__max_features': None, 'ridgereg__solver': 'auto', 'trans__col_name': 'text', 'cvec__preprocessor': None, 'ridgereg__normalize': False, 'tfvec': TfidfTransformer(norm=u'l2', smooth_idf=True, sublinear_tf=False,\n",
      "         use_idf=True), 'tfvec__use_idf': True, 'cvec__stop_words': None, 'ridgereg': Ridge(alpha=2, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001), 'ridgereg__copy_X': True, 'steps': [('trans', ColumnSelectTransformer(col_name='text')), ('cvec', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)), ('tfvec', TfidfTransformer(norm=u'l2', smooth_idf=True, sublinear_tf=False,\n",
      "         use_idf=True)), ('ridgereg', Ridge(alpha=2, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001))], 'cvec__strip_accents': None, 'trans': ColumnSelectTransformer(col_name='text')}\n"
     ]
    }
   ],
   "source": [
    "#normalized_est.fit(mydata, mystars) \n",
    "print(grid_search.best_estimator_.get_params())\n",
    "best_q2 = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score:  1.19426940833\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "score('nlp__normalized_model', best_q2.predict) #normalized_est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3. bigram_model\n",
    "In a bigram model, we'll consider both single words and pairs of consecutive words that appear.  This is going to be a much higher dimensional problem (large $p$) so you should be careful about overfitting.\n",
    "\n",
    "Sometimes, reducing the dimension can be useful.  Because we are dealing with a sparse matrix, we have to use [TruncatedSVD](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD).  If we reduce the dimensions, we can use a more sophisticated models than linear ones.\n",
    "\n",
    "As before, memory problems can crop up due to the engineering constraints. Playing with the number of features, using the `HashingVectorizer`, incorporating `min_df` and `max_df` limits, and handling stop-words in some way are all methods of addressing this issue. If you are using `CountVectorizer`, it is possible to run it with a fixed vocabulary (based on a training run, for instance). Check the documentation.\n",
    "\n",
    "**A side note on multi-stage model evaluation:** When your model consists of a pipeline with several stages, it can be worthwhile to evaluate which parts of the pipeline have the greatest impact on the overall accuracy (or other metric) of the model. This allows you to focus your efforts on improving the important algorithms, and leaving the rest \"good enough\".\n",
    "\n",
    "One way to accomplish this is through ceiling analysis, which can be useful when you have a training set with ground truth values at each stage. Let's say you're training a model to extract image captions from websites and return a list of names that were in the caption. Your overall accuracy at some point reaches 70%. You can try manually giving the model what you know are the correct image captions from the training set, and see how the accuracy improves (maybe up to 75%). Alternatively, giving the model the perfect name parsing for each caption increases accuracy to 90%. This indicates that the name parsing is a much more promising target for further work, and the caption extraction is a relatively smaller factor in the overall performance.\n",
    "\n",
    "If you don't know the right answers at different stages of the pipeline, you can still evaluate how important different parts of the model are to its performance by changing or removing certain steps while keeping everything else constant. You might try this kind of analysis to determine how important adding stopwords and stemming to your NLP model actually is, and how that importance changes with parameters like the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score:  1.19495328603\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "score('nlp__bigram_model', best_q2.predict) #bigram_est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 4. food_bigrams\n",
    "Look over all reviews of restaurants.  You can determine which businesses are restaurants by looking in the `yelp_train_academic_dataset_business.json.gz` file from the ml project or downloaded below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://dataincubator-course/mldata/yelp_train_academic_dataset_business.json.gz to ./yelp_train_academic_dataset_business.json.gz\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 sync s3://dataincubator-course/mldata/ . --exclude '*' --include 'yelp_train_academic_dataset_business.json.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import ujson as json\n",
    "with gzip.open('yelp_train_academic_dataset_business.json.gz') as f:\n",
    "    business_data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Restaurants']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "business_data[1]['categories']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Each row of this file corresponds to a single business.  The category key gives a list of categories for each; take all where \"Restaurants\" appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "restaurant_ids = [a for a in business_data if 'Restaurants' in a['categories']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "assert len(restaurant_ids) == 12876"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RgDg-k9S5YD_BaxMckifkg\n",
      "vcNAWiLM4dR7D2nwwJ7nCA\n",
      "12876\n",
      "12876\n"
     ]
    }
   ],
   "source": [
    "bid = restaurant_ids[3]['business_id']\n",
    "bidrev = data[3]['business_id']\n",
    "print bid\n",
    "print bidrev\n",
    "data[3]['review_id']\n",
    "bids = [ bid['business_id'] for bid in restaurant_ids ]\n",
    "bids_set = set(bids)\n",
    "print len(bids)\n",
    "print len(bids_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "restaurant_reviews = [ d['text'] for d in data if d['business_id'] in bids_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### The \"business_id\" here is the same as in the review data.  Use this to extract the review text for all reviews of restaurants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Very nice and clean place to have breakfast or lunch', u'I eat here regularily because it is consistently good.  Very large menu which makes it tough to choose but never disappointed!', u\"Unlimited hot coffee. I don't have any recollection of what I ate....\\n\\nI wanted t give it four stars, but that would be a very generous review for a meal I cannot recall.\\n\\nI know it was decently good, and it's like the only restaurant in Deforest.\\n\\nFor that, I give it four stars.\", u\"Homestyle cooking at its best, dnt go to culvers, McDonald's or whatever trash is right off the hwy, head a bit further down u will find this gem, one of the reasons I love this place so much? They dnt play annoying music, scratch that, they don't play ANY music. People working here are super nice as to be expected. If I owned this joint, I would invest in a new bigger sign, as I drove right by this place and almost missed it twice.\", u'Nice simple homey diner. Very friendly staff, huge family friendly menu, salad bar. If you are on the road this beats the same old options.', u\"Love their breakfast menu & they have friendly waitstaff. We go there nearly every Friday for the last 4 yrs.  I love that they don't play any music.  It is quiet, conversational, and comfortable.\", u'Went here for lunch with two co-workers, checked Yelp before we went and the reviews were very positive so we decided to give it a try.\\n\\nYikes! How can the reviews be 4-5 stars and the service at best \"average\", the food was terrible and myself and my co-workers all agreed, this restaurant doesn\\'t rate two stars.\\n\\nI had one of the special Friday offerings, Smelt. It came with a salad (very average but fresh), fries (ordinary) and the Smelt. After a 20-25 minute wait our server came with our orders. I looked at my plate and was shocked! There was 12-15 so-called Smelt about 3\" long and deep fried. They looked like little fat cigarettes! I bit into one and it was all batter, I tried another and it was the same. I had deep fried batter for lunch!!\\nI haven\\'t had a meal this bad since I got out of the Navy! \\n\\n My co-workes meals were not much better, one had Chicken Fried Steak and he said it was made out of the box. My other co-worker had the Cod and he said it was mushy and brown colored! YUK! \\n\\nI hate to write such a negative review but this restaurant earned \"one\" star. Maybe they have great breakfast! I hope they do because their lunches are sub-par. \\n\\nThere are other restaurants on this street/highway, I\\'d give them a try before I stopped here. I know my co-workers and I won\\'t be coming back.', u'Huge menu, fresh tasty food, reasonable prices.  Beats a chain highway place for sure. The chicken dumpling soup alone is worth a trip (fresh made every day and it caused my vegetable-phobic daughter to eat carrots and celery of her own voltion!);  the steak-cut French fries were great and the coconut cream pie was wonderful.', u'Food was very good. I had the Reuben sandwich. My only suggestion is that the waitress should check back more often to see if we need anything. Otherwise, I recommend it.', u'My boyfriend and I came here to have breakfast after New Years. We weren\\'t expecting much being not at our best either. \\n\\nThe service, as mentioned several times in yelp, is really slow. My drink was forgotten about and the next time our waitress came around she was serving our meal about 30-40 minutes from taking our order. The restaurant was probably half full of people. My boyfriends meal was mediocre \"at best\" as he put it but my meal was worse. My bacon and egg omelette was burnt and the cheese was scarce. \\n\\nAt the end of our meal, our server still charged us for the missing drink but they were kind enough to take it off the bill, that is while we waited 10 minutes standing at the register. I wouldn\\'t go back to this restaurant because I think my time is worth more than they treated it but could be nice for those who think differently.', u'Nice clean place with a very friendly staff.  Great homemade chicken dumpling soup and a very tasty club sandwich.  Decor is outdated, but not sloppy. Good local place to bring the family.', u\"Californians are all about the In-N-Out, where you can get your burger animal style. And the Midwest is all about Culver's, where the Butter Burger is the thing to order.\\n\\nC. and I ate lunch here on our way between Milwaukee and Mosinee before going to my folks for Christmas. I got a cheddar burger with fries, and C. got the two-piece chicken dinner with mashed potatoes and green beans.\\n\\nThat's one of the things I love about Culver's. Yes, it's a fast food restaurant. But they also do family restaurant food well. The chicken was good and the green beans really fresh, C. said. Fast food you can feel good about.\\n\\nI also got a frozen custard, their flavor of the day: Swiss almond Christmas or something like that. Chocolate custard with slivered almonds. Yummo.\", u\"I wanted ice cream so my husband pulled off the highway at this Culver's. We opted to get the flavor of the day (cookie dough craving). I was disappointed to find that it was a chocolate custard with TONS of cookie dough chunks because I like custard and not just the mix-ins, etc. but it was overall a decent sundae. I'll just stick to vanilla next time!\", u\"Next to Chick-Fil-A, Culver's is my fav fast food chain.  Rarely is a snowboarding trip complete without Culver's to top off the day.\\n\\nThe butter burgers hit the spot (though mine was heavy on the red onions; good thing those are easily removeable), and the fried cheese curds are to die for.  You don't find those floating around just anywhere.  But if there is anything you MUST get, it's a cup of the frozen custard.  Damn, that's good stuff.\", u\"Go-to meal: cheddar butter burger, fried cheese curds, and flavor of the day custard.\\n\\nThe difference between a deluxe burger and cheddar butter burger is that in the deluxe, you'll get American cheese.  And you have to pay 25 cents extra to add lettuce, tomato, etc. to the cheddar butter burger.  File that away in your memory bank.\", u\"Really good stuff!! The custard was great! I loved it and when I am back in the old USA I can only hope I will cross paths with another culvers.\\n\\nYou know what the thing is with culvers? You actually don't need that much, it's sort of like eating cake batter or cookie dough. You can only have so much before you start getting that sickly feeling and start stirring that plastic spoon around and staring down the custard and wondering why you dont wanna finish. Yeah it's custard and it's heavy especially when you add every kind of peanut butter chocholate bar to the mix.Let me tell you those first five spoonfuls are pure goodness then it goes downhill. Less is more with this stuff let me tell you! Next time I am getting the smallest size, and I will be far more pleased and not guilty I threw out a mega size bucket. Hahaha!\", u\"Love it!!!!! Love it!!!!!! love it!!!!!!!   Who doesn't love Culver's!\", u'Everything was great except for the burgers they are greasy and very charred compared to other stores.', u'I really like both Chinese restaurants in town.  This one has outstanding crab rangoon.  Love the chicken with snow peas and mushrooms and General Tso Chicken.  Food is always ready in 10 minutes which is accurate.  Good place and they give you free pop.', u'Above average takeout with friendly staff. The sauce on the pan fried noodle is tasty. Dumplings are quite good.', u\"We order from Chang Jiang often and have never been disappointed.  The menu is huge, and can accomodate anyone's taste buds.  The service is quick, usually ready in 10 minutes.\", u\"What could I possibly say about the Beach House Restaurant and Lounge?  I guess I could think of a few things.\\n\\nI went there today for the first time and was dazzled by the charm it flaunted, while still maintaining the gritty edge of a backwoods Wisconsin eatery.  Let's not get it twisted folks, we're not breaking gastronomic ground here.  What we are breaking are your misconceptions of what a restaurant can be whose parking lot is largely composed of Chevy and Dodge trucks, and whose front, snow-covered, lawn is full of snowmobiles.  Deal with it kids, this is that slice of Midwest Americana you've been looking for in any number of wayside eateries in rural America.  Fish fry anyone?\\n\\nTo begin, let me explain that one of the many draws for the Beach House's patrons is the fact that it is still legal to smoke indoors in McFarland.  I stepped inside and was immediately made aware of this fact as plumes of smoke took up residence in my clothes and nose.  \\n\\nDo not fear, however, there is a separate dining room that is nonsmoking.  And man was this separate dining room packed (Superbowl Sunday).  Packed with large women squeezing into convention center chairs.  Packed with waitresses who wore skin tight white Jordache jeans, giving you a fleeting sense of their giving nature.  Packed with the eager anticipation of an entire day spent watching football and football-related activities: drooling, eating, drinking, and smoking.  God bless America.\\n\\nThe AM dish to get here is the Walleye and eggs breakfast.  I would tend to agree, although their limited selection of omelets is made marginally more tasty by the addition of broccoli (you heard me punk).  As breakfast wore on, the dining room filled, emptied, and filled again; the waitstaff eagerly setting new places, removing paper place settings, calling people darlin' and hon'(always appreciated).\\n\\nSo give it to me hard backwoods Wisconsin.  Give me every sinew of your being: your ice fishing, snowmobiling, smoking, bloody Mary drinking, being.  Just be.  And give me a friggin' huge piece of deep fried walleye to help me digest the spectacle, with scrambled eggs on the side.  And don't you dare forget the tartar sauce.\", u'Service was terrible! I will never return!!! We waited in the bar and were never greeted or spoken too. People walked in after us and were seated. Not sure what their structure is for seating, but they suck at it. I will not be back.', u'This place is pretty dumpy - \"Corinthian leather\" chairs, crappy tables...\\nWhat is worth going for is the walleye and eggs served on weekends.  It is a HUGE portion of walleye, lighty fried, served with eggs cooked to order, potatoes and toast.  All this for something like $10.  Totally worth a side trip if you are in Madison.', u\"Amazing Friday night FISH FRY! I got out a lot for fish fry's in Madison and the surrounding area and this still remains my favorite. I usually get the Cod dinner, which is always cooked perfect, not too soggy or too crispy, which comes with a side AND a soup or salad for only $9.95. I always recommend the Beach House when someone is looking for a GREAT fish fry! :)\", u\"For being surrounded by two lakes, Madison doesn't do a great job of having places to drink on the water. Head down to McFarland and check out the beach house. It's like a dive bar slash restaurant right on the lake. Tons of windows. Not a bad place.\\n\\nCan't comment on the food. Drink specials were solid.\", u'I have not Yelped in quite a while and to all my fans (all none of you)...sorry.  So I will start on a positive note...\\n\\nSo I have been to the Beach House so many times I feel dumb reviewing it but I am so deal with it and go there.\\n\\nIf you are going for drinks...do it.  The clientele is awesome and you will always hear some great stories.  But this review is for the food.  ALWAYS AMAZING!  We went for breakfast the other day for the first time in a long time and it was as good as I remember.  The Eggs Benedict are just delicious and something I could eat every day and they make one hell of a Bloody Mary (so I am told by my wife since I do not like them) if your interest in that.\\n\\nSo if you want some good breakfast on the weekend and in the McFarland area...Breach House it is!', u'Nasty & Dirty.  Food is okay.  They do a great steak.  The place is full of regulars drinking cheap beer.', u'We had dinner at the Beach House on Friday on our way up to northern Wisconsin based on reviews on Yelp.  If you are looking for good fish fry the Beach House has it.  I had the perch which was great, my bf had the walleye which i think had an even better flavor.  The fish deserves 5 stars.  Everything else a 3 star.  I would give 3 1/2 stars if that were an option.  Would definitely go back.', u'I really wanted to like this place but it wasn\\'t what we were looking for.\\n\\nThey have a great spot on the lake but it is a pretty dumpy little fish shanty.   If that is what you are looking for, you will love it.  There were a bunch of locals there having a good time and if we were there with a group of our friends it might be ok for a few beers.  But still way way too crowded.  Like sardines in a can.\\nThere was no one to greet us so we just took a table by the window.   Then we waited and waited.  Then someone came over and said hi.  Then we waited and waited.  I was about to get up and leave when someone came over and said, \"oh, you haven\\'t been waited on yet?\"  So she then tried to help things along.   \\nThe menu was average for sandwiches but overpriced, we felt, for entrees as we ordered the Ahi Tuna and the Bluegill.   The Tuna was very good and perhaps appropriately priced but the Bluegill was not proportionately enough.  It was made well enough and had ok flavor but should have been a few dollars less.\\nSalad was ok.  At least had a variety of greens but still needed a little more.\\nIf I ever went back it would be for a few beers and the view, that\\'s it.\\nIt\\'s too bad because we really need some nice places on the lake and with all the lakes around here there aren\\'t as many lakeside restaurants as you would expect.']\n"
     ]
    }
   ],
   "source": [
    "print restaurant_reviews[20:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "assert len(restaurant_reviews) == 574278"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import dill\n",
    "dill.dump(restaurant_reviews, open('restaurant_reviews.dill', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import dill\n",
    "restaurant_reviews = dill.load(open('restaurant_reviews.dill', 'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We want to find collocations --- that is, bigrams that are \"special\" and appear more often than you'd expect from chance. We can think of the corpus as defining an empirical distribution over all ngrams.  We can find word pairs that are unlikely to occur consecutively based on the underlying probability of their words. Mathematically, if $p(w)$ be the probability of a word $w$ and $p(w_1 w_2)$ is the probability of the bigram $w_1 w_2$, then we want to look at word pairs $w_1 w_2$ where the statistic\n",
    "\n",
    "  $$ \\frac{p(w_1 w_2)}{p(w_1) p(w_2)} $$\n",
    "\n",
    "is high.  ** Return the top 100 (mostly food) bigrams with this statistic with the 'right' prior factor** (see below).\n",
    "\n",
    " Estimating the probabilities is simply a matter of counting, and there are number of approaches that will work.  \n",
    " One is to use one of the tokenizers to count up how many times each word and each bigram appears in each review, and then sum those up over all reviews.  \n",
    "#### You might want to know that the `CountVectorizer` has a `.get_feature_names()` method which gives the string associated with each column.  \n",
    "\n",
    "(Question for thought: Why doesn't the `HashingVectorizer` have a similar method?)\n",
    "\n",
    " *Questions:* This statistic is a ratio and problematic when the denominator is small.  We can fix this by applying  Bayesian smoothing to $p(w)$ (i.e. mixing the empirical distribution with the uniform distribution over the vocabulary).\n",
    "\n",
    " 1. How does changing this smoothing parameter affect the word pairs you get qualitatively?\n",
    "\n",
    " 2. We can interpret the smoothing parameter as adding a constant number of occurrences of each word to our distribution.  Does this help you determine set a reasonable value for this 'prior factor'?\n",
    "\n",
    " 3. For fun: also check out [Amazon's Statistically Improbable Phrases](http://en.wikipedia.org/wiki/Statistically_Improbable_Phrases).\n",
    "\n",
    " *Implementation note:*\n",
    " As you adjust the size of the Bayesian smoothing parameter, you will notice first nonsense phrases being removed and then legitimate bigrams being removed, leaving you with only generic bigrams.  The goal is to find a value of the smoothing parameter between these two transitions.\n",
    "\n",
    " The reference solution is not an aggressive filterer: it errors in favor of leaving apparently nonsensical words. On further consideration, many of these are actually somewhat meaningful. The smoothing parameter chosen in the reference solution is equivalent to giving each word ** 90**  previous appearances prior to considering this data.  This was chosen by generating a list of bigrams for a range of smoothing parameters and seeing how many of the bigrams were shared between neighboring values.  When the shared fraction reached 95%, we judged the solution to have converged.  Note that `min_df` should not be set too high, where it could exclude these borderline words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "assert len(restaurant_reviews) == 574278"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "counter_single = CountVectorizer(ngram_range=(1,1))\n",
    "\n",
    "counts1 = counter_single.fit_transform(restaurant_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "counter_double = CountVectorizer(analyzer='word',ngram_range=(2,2),min_df=20)\n",
    "\n",
    "counts2 = counter_double.fit_transform(restaurant_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(574278, 173736)\n",
      "(574278, 259564)\n",
      "  (0, 120414)\t1\n",
      "  (0, 74826)\t1\n",
      "  (0, 62518)\t1\n",
      "  (0, 245180)\t1\n",
      "  (0, 249228)\t1\n",
      "  (0, 246115)\t1\n",
      "  (0, 218191)\t1\n",
      "  (0, 150454)\t1\n",
      "  (0, 100121)\t1\n",
      "  (0, 48909)\t1\n",
      "  (0, 124987)\t1\n",
      "  (0, 245348)\t1\n",
      "  (0, 169585)\t1\n",
      "  (0, 179034)\t1\n",
      "  (0, 216573)\t1\n",
      "  (0, 21600)\t1\n",
      "  (0, 59926)\t1\n",
      "  (0, 232173)\t1\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print counts1.shape\n",
    "print counts2.shape\n",
    "#print counts.toarray() \n",
    "#print counts1.toarray()#dense matrix\n",
    "print counts2[0]\n",
    "print type(counts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 173736)\n"
     ]
    }
   ],
   "source": [
    "sum_count1 = counts1.sum(axis=0)\n",
    "print sum_count1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 259564)\n"
     ]
    }
   ],
   "source": [
    "sum_count2 = counts2.sum(axis=0)\n",
    "print sum_count2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11129   623     1 ...,     1     1     1]]\n",
      "173736\n",
      "259564\n"
     ]
    }
   ],
   "source": [
    "print sum_count1\n",
    "cut =0\n",
    "ctlist1 = sum_count1.tolist()[0]\n",
    "ctlist2 = sum_count2.tolist()[0]\n",
    "#for c in sum_count1.tolist()[0]:\n",
    "#    print c\n",
    "#    cut += 1\n",
    "#    if cut >10:\n",
    "#        break\n",
    "print len(ctlist1)\n",
    "print len(ctlist2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "names1 = counter_single.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "names2 = counter_double.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'1666666', u'1668', u'167', u'1677', u'168'] [u'21 to', u'21 we', u'21 year', u'21 years', u'215 and']\n",
      "173736\n",
      "259564\n"
     ]
    }
   ],
   "source": [
    "print names1[1100:1105], names2[1100:1105]\n",
    "print len(names1)\n",
    "print len(names2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dic1 = dict(zip(names1, ctlist1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'fawn', 22L)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic1.items()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dic2 = dict(zip(names2, ctlist2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dic2p = dic2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "they don 10281 [u'they', u'don']\n",
      "to find 19466 [u'to', u'find']\n",
      "customer service 12061 [u'customer', u'service']\n",
      "of their 21214 [u'of', u'their']\n",
      "delicious the 9531 [u'delicious', u'the']\n",
      "restaurant is 12360 [u'restaurant', u'is']\n",
      "the beef 10174 [u'the', u'beef']\n",
      "out the 18009 [u'out', u'the']\n",
      "and we 40377 [u'and', u'we']\n",
      "on their 10579 [u'on', u'their']\n",
      "at this 15138 [u'at', u'this']\n",
      "and not 24120 [u'and', u'not']\n",
      "to have 30729 [u'to', u'have']\n",
      "the menu 60410 [u'the', u'menu']\n",
      "was pretty 26432 [u'was', u'pretty']\n",
      "salad and 11274 [u'salad', u'and']\n",
      "were very 15429 [u'were', u'very']\n",
      "to order 24760 [u'to', u'order']\n",
      "the burger 11203 [u'the', u'burger']\n",
      "our waitress 10353 [u'our', u'waitress']\n",
      "this time 10656 [u'this', u'time']\n",
      "worth the 16540 [u'worth', u'the']\n",
      "but we 15448 [u'but', u'we']\n",
      "which was 42884 [u'which', u'was']\n",
      "ve ever 17441 [u've', u'ever']\n",
      "here is 13715 [u'here', u'is']\n",
      "is just 12840 [u'is', u'just']\n",
      "the restaurant 44131 [u'the', u'restaurant']\n",
      "you have 24793 [u'you', u'have']\n",
      "was excellent 13556 [u'was', u'excellent']\n",
      "menu is 12089 [u'menu', u'is']\n",
      "that was 38440 [u'that', u'was']\n",
      "want to 35146 [u'want', u'to']\n",
      "go here 9465 [u'go', u'here']\n",
      "our table 14049 [u'our', u'table']\n",
      "not to 12278 [u'not', u'to']\n",
      "and that 16961 [u'and', u'that']\n",
      "like the 28104 [u'like', u'the']\n",
      "was the 58112 [u'was', u'the']\n",
      "plenty of 11509 [u'plenty', u'of']\n",
      "and service 11662 [u'and', u'service']\n",
      "the portions 10044 [u'the', u'portions']\n",
      "me the 9079 [u'me', u'the']\n",
      "hard to 13651 [u'hard', u'to']\n",
      "loved the 12031 [u'loved', u'the']\n",
      "to my 18450 [u'to', u'my']\n",
      "were good 12235 [u'were', u'good']\n",
      "will be 30465 [u'will', u'be']\n",
      "it came 10989 [u'it', u'came']\n",
      "for breakfast 10864 [u'for', u'breakfast']\n",
      "and just 11440 [u'and', u'just']\n",
      "and my 24917 [u'and', u'my']\n",
      "should have 9911 [u'should', u'have']\n",
      "instead of 13140 [u'instead', u'of']\n",
      "and friendly 10766 [u'and', u'friendly']\n",
      "say that 11164 [u'say', u'that']\n",
      "in and 13455 [u'in', u'and']\n",
      "what you 15899 [u'what', u'you']\n",
      "of course 14972 [u'of', u'course']\n",
      "as we 11546 [u'as', u'we']\n",
      "enough to 14124 [u'enough', u'to']\n",
      "at the 94117 [u'at', u'the']\n",
      "for our 15826 [u'for', u'our']\n",
      "are very 10187 [u'are', u'very']\n",
      "used to 17515 [u'used', u'to']\n",
      "we have 14941 [u'we', u'have']\n",
      "it was 277540 [u'it', u'was']\n",
      "bit of 11501 [u'bit', u'of']\n",
      "good the 23602 [u'good', u'the']\n",
      "good but 30902 [u'good', u'but']\n",
      "not be 10765 [u'not', u'be']\n",
      "the meal 16729 [u'the', u'meal']\n",
      "our waiter 10947 [u'our', u'waiter']\n",
      "about this 15503 [u'about', u'this']\n",
      "we were 91689 [u'we', u'were']\n",
      "had the 81958 [u'had', u'the']\n",
      "was awesome 9747 [u'was', u'awesome']\n",
      "like to 12008 [u'like', u'to']\n",
      "we ordered 33426 [u'we', u'ordered']\n",
      "of our 14889 [u'of', u'our']\n",
      "was just 27285 [u'was', u'just']\n",
      "las vegas 23612 [u'las', u'vegas']\n",
      "it so 9427 [u'it', u'so']\n",
      "got the 29799 [u'got', u'the']\n",
      "to eat 42846 [u'to', u'eat']\n",
      "tasty and 9019 [u'tasty', u'and']\n",
      "has been 13204 [u'has', u'been']\n",
      "when the 12048 [u'when', u'the']\n",
      "come back 21677 [u'come', u'back']\n",
      "to come 21403 [u'to', u'come']\n",
      "enjoyed the 9915 [u'enjoyed', u'the']\n",
      "my wife 21442 [u'my', u'wife']\n",
      "chicken and 14277 [u'chicken', u'and']\n",
      "so that 9632 [u'so', u'that']\n",
      "love the 24264 [u'love', u'the']\n",
      "to me 18932 [u'to', u'me']\n",
      "time we 14631 [u'time', u'we']\n",
      "for lunch 27739 [u'for', u'lunch']\n",
      "the wait 17724 [u'the', u'wait']\n",
      "decided to 28959 [u'decided', u'to']\n",
      "to see 20652 [u'to', u'see']\n",
      "as the 22543 [u'as', u'the']\n",
      "the day 10766 [u'the', u'day']\n",
      "the kitchen 11715 [u'the', u'kitchen']\n",
      "and all 10330 [u'and', u'all']\n",
      "so good 18423 [u'so', u'good']\n",
      "my husband 28461 [u'my', u'husband']\n",
      "the price 27165 [u'the', u'price']\n",
      "if it 14484 [u'if', u'it']\n",
      "the server 15623 [u'the', u'server']\n",
      "we had 63026 [u'we', u'had']\n",
      "food but 10348 [u'food', u'but']\n",
      "was really 25959 [u'was', u'really']\n",
      "but was 11224 [u'but', u'was']\n",
      "and some 12124 [u'and', u'some']\n",
      "nice and 18874 [u'nice', u'and']\n",
      "every time 13934 [u'every', u'time']\n",
      "for that 9273 [u'for', u'that']\n",
      "and it 86395 [u'and', u'it']\n",
      "rice and 9283 [u'rice', u'and']\n",
      "trying to 12297 [u'trying', u'to']\n",
      "the most 17062 [u'the', u'most']\n",
      "the night 9310 [u'the', u'night']\n",
      "can say 9118 [u'can', u'say']\n",
      "make it 11547 [u'make', u'it']\n",
      "was my 11014 [u'was', u'my']\n",
      "had to 42733 [u'had', u'to']\n",
      "prices are 13646 [u'prices', u'are']\n",
      "the waiter 14194 [u'the', u'waiter']\n",
      "they had 26051 [u'they', u'had']\n",
      "all in 10862 [u'all', u'in']\n",
      "order the 12119 [u'order', u'the']\n",
      "have been 46336 [u'have', u'been']\n",
      "and have 22115 [u'and', u'have']\n",
      "the fries 11602 [u'the', u'fries']\n",
      "it to 18110 [u'it', u'to']\n",
      "of this 19515 [u'of', u'this']\n",
      "will not 10152 [u'will', u'not']\n",
      "times and 9504 [u'times', u'and']\n",
      "husband and 9420 [u'husband', u'and']\n",
      "great food 16515 [u'great', u'food']\n",
      "at least 25248 [u'at', u'least']\n",
      "but that 13123 [u'but', u'that']\n",
      "great and 15046 [u'great', u'and']\n",
      "to wait 15651 [u'to', u'wait']\n",
      "in your 10136 [u'in', u'your']\n",
      "since it 9521 [u'since', u'it']\n",
      "fast food 9749 [u'fast', u'food']\n",
      "is really 12702 [u'is', u'really']\n",
      "next to 12240 [u'next', u'to']\n",
      "not sure 14383 [u'not', u'sure']\n",
      "their food 10446 [u'their', u'food']\n",
      "is good 23587 [u'is', u'good']\n",
      "wait to 11899 [u'wait', u'to']\n",
      "on my 20548 [u'on', u'my']\n",
      "ll be 13646 [u'll', u'be']\n",
      "going back 10286 [u'going', u'back']\n",
      "because the 14228 [u'because', u'the']\n",
      "to it 12319 [u'to', u'it']\n",
      "of all 10723 [u'of', u'all']\n",
      "that we 21521 [u'that', u'we']\n",
      "come here 16020 [u'come', u'here']\n",
      "this location 13826 [u'this', u'location']\n",
      "so you 11017 [u'so', u'you']\n",
      "bar and 10056 [u'bar', u'and']\n",
      "selection of 12423 [u'selection', u'of']\n",
      "also had 10173 [u'also', u'had']\n",
      "but the 68234 [u'but', u'the']\n",
      "have had 14276 [u'have', u'had']\n",
      "he was 15881 [u'he', u'was']\n",
      "the whole 13869 [u'the', u'whole']\n",
      "but you 9172 [u'but', u'you']\n",
      "me to 12956 [u'me', u'to']\n",
      "the way 19688 [u'the', u'way']\n",
      "ask for 13775 [u'ask', u'for']\n",
      "this is 81444 [u'this', u'is']\n",
      "off the 20932 [u'off', u'the']\n",
      "in this 12246 [u'in', u'this']\n",
      "there and 9217 [u'there', u'and']\n",
      "the other 34016 [u'the', u'other']\n",
      "this was 33077 [u'this', u'was']\n",
      "went to 16912 [u'went', u'to']\n",
      "for it 13917 [u'for', u'it']\n",
      "the staff 33302 [u'the', u'staff']\n",
      "it the 26322 [u'it', u'the']\n",
      "our server 22580 [u'our', u'server']\n",
      "you know 10162 [u'you', u'know']\n",
      "all you 9971 [u'all', u'you']\n",
      "was very 60441 [u'was', u'very']\n",
      "we also 11964 [u'we', u'also']\n",
      "here the 10101 [u'here', u'the']\n",
      "it up 10174 [u'it', u'up']\n",
      "very good 41455 [u'very', u'good']\n",
      "cheese and 12979 [u'cheese', u'and']\n",
      "is one 12752 [u'is', u'one']\n",
      "even though 14052 [u'even', u'though']\n",
      "full of 9488 [u'full', u'of']\n",
      "the service 75830 [u'the', u'service']\n",
      "to our 12176 [u'to', u'our']\n",
      "back and 14050 [u'back', u'and']\n",
      "to do 13211 [u'to', u'do']\n",
      "highly recommend 11295 [u'highly', u'recommend']\n",
      "our food 14933 [u'our', u'food']\n",
      "know what 11342 [u'know', u'what']\n",
      "service was 61966 [u'service', u'was']\n",
      "here and 13984 [u'here', u'and']\n",
      "kind of 28060 [u'kind', u'of']\n",
      "and he 13925 [u'and', u'he']\n",
      "have never 9978 [u'have', u'never']\n",
      "ve had 23656 [u've', u'had']\n",
      "it had 14900 [u'it', u'had']\n",
      "it has 12122 [u'it', u'has']\n",
      "went here 10737 [u'went', u'here']\n",
      "the quality 14044 [u'the', u'quality']\n",
      "had great 9105 [u'had', u'great']\n",
      "not bad 10376 [u'not', u'bad']\n",
      "looking for 18561 [u'looking', u'for']\n",
      "to go 65263 [u'to', u'go']\n",
      "the place 55089 [u'the', u'place']\n",
      "the chicken 32393 [u'the', u'chicken']\n",
      "me and 12073 [u'me', u'and']\n",
      "and so 9003 [u'and', u'so']\n",
      "last night 9870 [u'last', u'night']\n",
      "friendly and 26534 [u'friendly', u'and']\n",
      "can get 16172 [u'can', u'get']\n",
      "came here 15629 [u'came', u'here']\n",
      "you don 13000 [u'you', u'don']\n",
      "we ve 10322 [u'we', u've']\n",
      "never had 10228 [u'never', u'had']\n",
      "it all 12290 [u'it', u'all']\n",
      "my order 9581 [u'my', u'order']\n",
      "you ll 13950 [u'you', u'll']\n",
      "server was 12905 [u'server', u'was']\n",
      "worth it 16732 [u'worth', u'it']\n",
      "to get 60504 [u'to', u'get']\n",
      "this restaurant 19707 [u'this', u'restaurant']\n",
      "it would 16248 [u'it', u'would']\n",
      "the side 12842 [u'the', u'side']\n",
      "the shrimp 9431 [u'the', u'shrimp']\n",
      "was ok 12778 [u'was', u'ok']\n",
      "way to 14646 [u'way', u'to']\n",
      "were not 10429 [u'were', u'not']\n",
      "up the 12943 [u'up', u'the']\n",
      "food the 12062 [u'food', u'the']\n",
      "place is 56221 [u'place', u'is']\n",
      "the street 9337 [u'the', u'street']\n",
      "and they 52292 [u'and', u'they']\n",
      "and then 18031 [u'and', u'then']\n",
      "great the 10742 [u'great', u'the']\n",
      "if the 9523 [u'if', u'the']\n",
      "coming back 9899 [u'coming', u'back']\n",
      "mexican food 11509 [u'mexican', u'food']\n",
      "the sushi 12868 [u'the', u'sushi']\n",
      "it for 9317 [u'it', u'for']\n",
      "when you 23537 [u'when', u'you']\n",
      "my boyfriend 12310 [u'my', u'boyfriend']\n",
      "amount of 15278 [u'amount', u'of']\n",
      "the right 10369 [u'the', u'right']\n",
      "as it 14377 [u'as', u'it']\n",
      "most of 13020 [u'most', u'of']\n",
      "all the 50179 [u'all', u'the']\n",
      "there were 22197 [u'there', u'were']\n",
      "and great 10231 [u'and', u'great']\n",
      "the area 12948 [u'the', u'area']\n",
      "lots of 17346 [u'lots', u'of']\n",
      "fried rice 10062 [u'fried', u'rice']\n",
      "pizza and 9117 [u'pizza', u'and']\n",
      "it wasn 18420 [u'it', u'wasn']\n",
      "was on 9175 [u'was', u'on']\n",
      "is always 20372 [u'is', u'always']\n",
      "is great 30809 [u'is', u'great']\n",
      "the food 175539 [u'the', u'food']\n",
      "be back 29302 [u'be', u'back']\n",
      "the waitress 16835 [u'the', u'waitress']\n",
      "in vegas 38757 [u'in', u'vegas']\n",
      "ve never 9512 [u've', u'never']\n",
      "was also 14242 [u'was', u'also']\n",
      "for you 10520 [u'for', u'you']\n",
      "was bit 11189 [u'was', u'bit']\n",
      "my favorite 33802 [u'my', u'favorite']\n",
      "for dinner 20043 [u'for', u'dinner']\n",
      "of my 37153 [u'of', u'my']\n",
      "is pretty 15260 [u'is', u'pretty']\n",
      "you get 26093 [u'you', u'get']\n",
      "this place 192219 [u'this', u'place']\n",
      "is very 25568 [u'is', u'very']\n",
      "ordered the 52423 [u'ordered', u'the']\n",
      "the fact 10782 [u'the', u'fact']\n",
      "ve been 29366 [u've', u'been']\n",
      "food is 57655 [u'food', u'is']\n",
      "if they 13931 [u'if', u'they']\n",
      "in all 12531 [u'in', u'all']\n",
      "the bread 13504 [u'the', u'bread']\n",
      "make sure 11225 [u'make', u'sure']\n",
      "that they 31382 [u'that', u'they']\n",
      "an hour 9910 [u'an', u'hour']\n",
      "think it 9959 [u'think', u'it']\n",
      "couple of 13732 [u'couple', u'of']\n",
      "we went 20113 [u'we', u'went']\n",
      "it took 10871 [u'it', u'took']\n",
      "the end 10926 [u'the', u'end']\n",
      "staff was 11503 [u'staff', u'was']\n",
      "but this 15479 [u'but', u'this']\n",
      "to take 18385 [u'to', u'take']\n",
      "rest of 10292 [u'rest', u'of']\n",
      "won be 9633 [u'won', u'be']\n",
      "of the 251595 [u'of', u'the']\n",
      "feel like 11244 [u'feel', u'like']\n",
      "it not 18405 [u'it', u'not']\n",
      "up for 9330 [u'up', u'for']\n",
      "we decided 11508 [u'we', u'decided']\n",
      "you will 16128 [u'you', u'will']\n",
      "you can 68547 [u'you', u'can']\n",
      "too much 15468 [u'too', u'much']\n",
      "the manager 14129 [u'the', u'manager']\n",
      "around the 10208 [u'around', u'the']\n",
      "just the 9584 [u'just', u'the']\n",
      "very tasty 9341 [u'very', u'tasty']\n",
      "and their 10136 [u'and', u'their']\n",
      "it but 10662 [u'it', u'but']\n",
      "not the 20534 [u'not', u'the']\n",
      "to check 10103 [u'to', u'check']\n",
      "over the 17341 [u'over', u'the']\n",
      "the meat 18053 [u'the', u'meat']\n",
      "so much 16650 [u'so', u'much']\n",
      "would be 29656 [u'would', u'be']\n",
      "and had 27992 [u'and', u'had']\n",
      "to vegas 9375 [u'to', u'vegas']\n",
      "the prices 16645 [u'the', u'prices']\n",
      "the salad 11542 [u'the', u'salad']\n",
      "we could 9044 [u'we', u'could']\n",
      "and there 15922 [u'and', u'there']\n",
      "and you 22868 [u'and', u'you']\n",
      "was little 15286 [u'was', u'little']\n",
      "and she 12410 [u'and', u'she']\n",
      "staff is 13554 [u'staff', u'is']\n",
      "food and 40025 [u'food', u'and']\n",
      "for an 10638 [u'for', u'an']\n",
      "try the 21057 [u'try', u'the']\n",
      "if you 120923 [u'if', u'you']\n",
      "it and 19036 [u'it', u'and']\n",
      "to ask 11535 [u'to', u'ask']\n",
      "have the 14823 [u'have', u'the']\n",
      "very nice 14278 [u'very', u'nice']\n",
      "much better 10594 [u'much', u'better']\n",
      "the strip 21459 [u'the', u'strip']\n",
      "prime rib 9804 [u'prime', u'rib']\n",
      "top of 9999 [u'top', u'of']\n",
      "because it 19429 [u'because', u'it']\n",
      "was so 31093 [u'was', u'so']\n",
      "side of 18902 [u'side', u'of']\n",
      "the fish 12574 [u'the', u'fish']\n",
      "go back 32826 [u'go', u'back']\n",
      "are the 12725 [u'are', u'the']\n",
      "the last 13576 [u'the', u'last']\n",
      "get to 9814 [u'get', u'to']\n",
      "the decor 12504 [u'the', u'decor']\n",
      "everything was 13430 [u'everything', u'was']\n",
      "lot of 27815 [u'lot', u'of']\n",
      "was great 37565 [u'was', u'great']\n",
      "but it 60594 [u'but', u'it']\n",
      "but if 10687 [u'but', u'if']\n",
      "chips and 9342 [u'chips', u'and']\n",
      "place and 12406 [u'place', u'and']\n",
      "the best 92681 [u'the', u'best']\n",
      "ended up 14367 [u'ended', u'up']\n",
      "the rest 13155 [u'the', u'rest']\n",
      "about the 30047 [u'about', u'the']\n",
      "been to 21191 [u'been', u'to']\n",
      "did not 25242 [u'did', u'not']\n",
      "the next 13492 [u'the', u'next']\n",
      "if we 11789 [u'if', u'we']\n",
      "should be 10865 [u'should', u'be']\n",
      "fan of 15888 [u'fan', u'of']\n",
      "there was 41088 [u'there', u'was']\n",
      "in my 32954 [u'in', u'my']\n",
      "tasted like 9983 [u'tasted', u'like']\n",
      "and very 14500 [u'and', u'very']\n",
      "the first 28917 [u'the', u'first']\n",
      "were the 12067 [u'were', u'the']\n",
      "were seated 10389 [u'were', u'seated']\n",
      "is not 27310 [u'is', u'not']\n",
      "place was 22789 [u'place', u'was']\n",
      "in it 12082 [u'in', u'it']\n",
      "is so 13800 [u'is', u'so']\n",
      "the bar 35615 [u'the', u'bar']\n",
      "up to 17138 [u'up', u'to']\n",
      "there are 27980 [u'there', u'are']\n",
      "so many 11101 [u'so', u'many']\n",
      "drinks and 9532 [u'drinks', u'and']\n",
      "came with 16894 [u'came', u'with']\n",
      "my first 14034 [u'my', u'first']\n",
      "good food 16115 [u'good', u'food']\n",
      "had in 9511 [u'had', u'in']\n",
      "when it 14762 [u'when', u'it']\n",
      "had been 10652 [u'had', u'been']\n",
      "have to 49126 [u'have', u'to']\n",
      "some of 26892 [u'some', u'of']\n",
      "was not 35907 [u'was', u'not']\n",
      "so it 15026 [u'so', u'it']\n",
      "ice cream 15544 [u'ice', u'cream']\n",
      "all of 21878 [u'all', u'of']\n",
      "it out 13140 [u'it', u'out']\n",
      "up and 11118 [u'up', u'and']\n",
      "to the 118459 [u'to', u'the']\n",
      "from the 57126 [u'from', u'the']\n",
      "the time 18741 [u'the', u'time']\n",
      "for me 29923 [u'for', u'me']\n",
      "for my 25461 [u'for', u'my']\n",
      "recommend this 9875 [u'recommend', u'this']\n",
      "but not 29488 [u'but', u'not']\n",
      "and good 9024 [u'and', u'good']\n",
      "is my 11593 [u'is', u'my']\n",
      "out and 11184 [u'out', u'and']\n",
      "place to 36030 [u'place', u'to']\n",
      "have ever 13029 [u'have', u'ever']\n",
      "down the 11438 [u'down', u'the']\n",
      "it in 10727 [u'it', u'in']\n",
      "it it 9206 [u'it', u'it']\n",
      "it is 80640 [u'it', u'is']\n",
      "served with 10422 [u'served', u'with']\n",
      "this one 12671 [u'this', u'one']\n",
      "to make 26134 [u'to', u'make']\n",
      "give it 14258 [u'give', u'it']\n",
      "fact that 9573 [u'fact', u'that']\n",
      "able to 15985 [u'able', u'to']\n",
      "variety of 9921 [u'variety', u'of']\n",
      "that is 25934 [u'that', u'is']\n",
      "that it 27343 [u'that', u'it']\n",
      "of us 21473 [u'of', u'us']\n",
      "let me 9629 [u'let', u'me']\n",
      "we did 10073 [u'we', u'did']\n",
      "as good 13231 [u'as', u'good']\n",
      "she was 14162 [u'she', u'was']\n",
      "one of 70773 [u'one', u'of']\n",
      "to us 10442 [u'to', u'us']\n",
      "asked for 12937 [u'asked', u'for']\n",
      "and will 10109 [u'and', u'will']\n",
      "and cheese 14053 [u'and', u'cheese']\n",
      "they do 13456 [u'they', u'do']\n",
      "there is 30636 [u'there', u'is']\n",
      "the pizza 18718 [u'the', u'pizza']\n",
      "of them 12095 [u'of', u'them']\n",
      "as well 43941 [u'as', u'well']\n",
      "love this 16576 [u'love', u'this']\n",
      "first time 23963 [u'first', u'time']\n",
      "are not 9479 [u'are', u'not']\n",
      "wanted to 23473 [u'wanted', u'to']\n",
      "is nice 9025 [u'is', u'nice']\n",
      "very friendly 13992 [u'very', u'friendly']\n",
      "be the 11917 [u'be', u'the']\n",
      "sauce and 10317 [u'sauce', u'and']\n",
      "the steak 12137 [u'the', u'steak']\n",
      "great service 14052 [u'great', u'service']\n",
      "we will 14313 [u'we', u'will']\n",
      "to this 21128 [u'to', u'this']\n",
      "because they 10700 [u'because', u'they']\n",
      "is the 62945 [u'is', u'the']\n",
      "well as 10543 [u'well', u'as']\n",
      "came out 19366 [u'came', u'out']\n",
      "which is 35271 [u'which', u'is']\n",
      "was nice 16001 [u'was', u'nice']\n",
      "into the 12413 [u'into', u'the']\n",
      "need to 16050 [u'need', u'to']\n",
      "because of 12503 [u'because', u'of']\n",
      "out of 45619 [u'out', u'of']\n",
      "is that 19621 [u'is', u'that']\n",
      "great place 14455 [u'great', u'place']\n",
      "place the 9382 [u'place', u'the']\n",
      "go to 32933 [u'go', u'to']\n",
      "back for 12083 [u'back', u'for']\n",
      "don know 14238 [u'don', u'know']\n",
      "with my 21966 [u'with', u'my']\n",
      "that you 16886 [u'that', u'you']\n",
      "back to 26624 [u'back', u'to']\n",
      "for us 10271 [u'for', u'us']\n",
      "on top 16088 [u'on', u'top']\n",
      "and this 15256 [u'and', u'this']\n",
      "the table 21857 [u'the', u'table']\n",
      "the pork 9167 [u'the', u'pork']\n",
      "it just 14232 [u'it', u'just']\n",
      "to say 24026 [u'to', u'say']\n",
      "there for 10214 [u'there', u'for']\n",
      "get the 23807 [u'get', u'the']\n",
      "service and 18937 [u'service', u'and']\n",
      "out to 12190 [u'out', u'to']\n",
      "you want 23628 [u'you', u'want']\n",
      "happy hour 29074 [u'happy', u'hour']\n",
      "can be 15678 [u'can', u'be']\n",
      "service is 30246 [u'service', u'is']\n",
      "our order 9106 [u'our', u'order']\n",
      "you are 35189 [u'you', u'are']\n",
      "next time 22312 [u'next', u'time']\n",
      "for the 132746 [u'for', u'the']\n",
      "was amazing 13341 [u'was', u'amazing']\n",
      "better than 19083 [u'better', u'than']\n",
      "in las 10733 [u'in', u'las']\n",
      "like it 17656 [u'like', u'it']\n",
      "not too 13762 [u'not', u'too']\n",
      "would have 27286 [u'would', u'have']\n",
      "for dessert 9083 [u'for', u'dessert']\n",
      "of it 18421 [u'of', u'it']\n",
      "going to 33009 [u'going', u'to']\n",
      "you re 44178 [u'you', u're']\n",
      "about it 13813 [u'about', u'it']\n",
      "by the 35662 [u'by', u'the']\n",
      "you like 10931 [u'you', u'like']\n",
      "up with 9880 [u'up', u'with']\n",
      "part of 12827 [u'part', u'of']\n",
      "the atmosphere 17937 [u'the', u'atmosphere']\n",
      "on our 9557 [u'on', u'our']\n",
      "good as 14675 [u'good', u'as']\n",
      "was in 14258 [u'was', u'in']\n",
      "food was 63753 [u'food', u'was']\n",
      "they have 60374 [u'they', u'have']\n",
      "place for 17721 [u'place', u'for']\n",
      "that this 10566 [u'that', u'this']\n",
      "they re 16498 [u'they', u're']\n",
      "to give 17117 [u'to', u'give']\n",
      "when they 9943 [u'when', u'they']\n",
      "ever had 22508 [u'ever', u'had']\n",
      "to sit 10528 [u'to', u'sit']\n",
      "menu and 12267 [u'menu', u'and']\n",
      "the same 34133 [u'the', u'same']\n",
      "my friends 14004 [u'my', u'friends']\n",
      "that the 40975 [u'that', u'the']\n",
      "on the 170072 [u'on', u'the']\n",
      "could have 14145 [u'could', u'have']\n",
      "we got 25855 [u'we', u'got']\n",
      "eat here 11447 [u'eat', u'here']\n",
      "but they 22412 [u'but', u'they']\n",
      "been here 14071 [u'been', u'here']\n",
      "is amazing 10623 [u'is', u'amazing']\n",
      "pretty good 27802 [u'pretty', u'good']\n",
      "at all 23570 [u'at', u'all']\n",
      "my friend 21908 [u'my', u'friend']\n",
      "the only 37473 [u'the', u'only']\n",
      "to be 95407 [u'to', u'be']\n",
      "the patio 10791 [u'the', u'patio']\n",
      "time to 12405 [u'time', u'to']\n",
      "they also 14225 [u'they', u'also']\n",
      "and the 226853 [u'and', u'the']\n",
      "can eat 10332 [u'can', u'eat']\n",
      "we are 11469 [u'we', u'are']\n",
      "close to 11560 [u'close', u'to']\n",
      "the one 11802 [u'the', u'one']\n",
      "on it 11936 [u'on', u'it']\n",
      "they were 58152 [u'they', u'were']\n",
      "to try 52752 [u'to', u'try']\n",
      "the owner 14885 [u'the', u'owner']\n",
      "place has 9538 [u'place', u'has']\n",
      "in the 180981 [u'in', u'the']\n",
      "for some 13028 [u'for', u'some']\n",
      "and delicious 11215 [u'and', u'delicious']\n",
      "will definitely 13833 [u'will', u'definitely']\n",
      "pretty much 9257 [u'pretty', u'much']\n",
      "here for 25355 [u'here', u'for']\n",
      "fresh and 19291 [u'fresh', u'and']\n",
      "so we 28448 [u'so', u'we']\n",
      "quality of 10771 [u'quality', u'of']\n",
      "good and 30860 [u'good', u'and']\n",
      "the buffet 10247 [u'the', u'buffet']\n",
      "for this 11781 [u'for', u'this']\n",
      "with some 12444 [u'with', u'some']\n",
      "and were 15401 [u'and', u'were']\n",
      "tried the 13826 [u'tried', u'the']\n",
      "more than 20328 [u'more', u'than']\n",
      "do not 13785 [u'do', u'not']\n",
      "was delicious 19889 [u'was', u'delicious']\n",
      "really good 27485 [u'really', u'good']\n",
      "of food 18012 [u'of', u'food']\n",
      "didn have 10643 [u'didn', u'have']\n",
      "delicious and 13617 [u'delicious', u'and']\n",
      "we didn 11186 [u'we', u'didn']\n",
      "definitely be 9872 [u'definitely', u'be']\n",
      "and was 36216 [u'and', u'was']\n",
      "the cheese 9330 [u'the', u'cheese']\n",
      "than the 13148 [u'than', u'the']\n",
      "they are 40493 [u'they', u'are']\n",
      "with the 96276 [u'with', u'the']\n",
      "like this 12276 [u'like', u'this']\n",
      "in town 16427 [u'in', u'town']\n",
      "try it 14502 [u'try', u'it']\n",
      "time and 11868 [u'time', u'and']\n",
      "when we 28459 [u'when', u'we']\n",
      "the sauce 13601 [u'the', u'sauce']\n",
      "the counter 10131 [u'the', u'counter']\n",
      "was good 50365 [u'was', u'good']\n"
     ]
    }
   ],
   "source": [
    "for k,v in dic2.items():\n",
    "    if v>9000:\n",
    "        print k,v, k.split(' ')\n",
    "    w = k.split(' ')\n",
    "    dic2p[k] = 1.0*v/(dic1[w[0]]+90.0)/(dic1[w[1]]+90.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "sorted_dic = OrderedDict(sorted(dic2p.items(), key=lambda t: -t[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ezzyujdouig4p gyb3pv_a 0.00277744233788\n",
      "hodge podge 0.00258620689655\n",
      "himal chuli 0.00251607492312\n",
      "hoity toity 0.00251198903859\n",
      "roka akor 0.00245098039216\n",
      "f_5_unx wrafcxuakbzrdw 0.00245043439519\n",
      "knick knacks 0.00243463533397\n",
      "reina pepiada 0.00242052023121\n",
      "cien agaves 0.00240286990659\n",
      "itty bitty 0.00236317627622\n",
      "khai hoan 0.00234192037471\n",
      "riff raff 0.00231061231226\n",
      "baskin robbins 0.00228190932216\n",
      "grana padano 0.00225076855512\n",
      "tutti santi 0.00224285268149\n",
      "ropa vieja 0.00219705659036\n",
      "gulab jamun 0.00218431454129\n",
      "pel meni 0.00216346153846\n",
      "ore ida 0.00216086434574\n",
      "laan xang 0.00213971778857\n",
      "dac biet 0.00213430408752\n",
      "rula bula 0.0021297353893\n",
      "hu tieu 0.00209930631617\n",
      "bandeja paisa 0.00207100591716\n",
      "tammie coe 0.00206388206388\n",
      "chicha morada 0.00204600752046\n",
      "alain ducasse 0.00202571094663\n",
      "dol sot 0.00200534759358\n",
      "itsy bitsy 0.00199724517906\n",
      "mille feuille 0.00198412698413\n",
      "feng shui 0.00198075834748\n",
      "hors oeuvres 0.00194298455168\n",
      "marche bacchus 0.00191161866931\n",
      "uuu uuu 0.00191112411976\n",
      "celine dion 0.00189670265538\n",
      "nanay gloria 0.00183598531212\n",
      "doon varna 0.00182641729982\n",
      "luc lac 0.0018261504748\n",
      "krispy kreme 0.00182170770406\n",
      "woonam jung 0.00181333333333\n",
      "perrier jouet 0.00181053043648\n",
      "deja vu 0.00180243720857\n",
      "molecular gastronomy 0.00179713392149\n",
      "puerto rican 0.00178302016184\n",
      "vice versa 0.00177672955975\n",
      "patatas bravas 0.00176911927701\n",
      "sais quoi 0.00176577808157\n",
      "cullen skink 0.0017624651771\n",
      "lloyd wright 0.00175764085539\n",
      "holyrood 9a 0.00175032465699\n",
      "pura vida 0.00174700875536\n",
      "lomo saltado 0.00172243770955\n",
      "nuoc mam 0.00171305081172\n",
      "wal mart 0.00170582967291\n",
      "dueling pianos 0.00170492348636\n",
      "valle luna 0.00169950559837\n",
      "bradley ogden 0.00168821598446\n",
      "rustler rooste 0.00168495636395\n",
      "avant garde 0.00167024576473\n",
      "honky tonk 0.00165256478054\n",
      "haricot vert 0.00164015089388\n",
      "kao tod 0.00162961050695\n",
      "irn bru 0.00162521198417\n",
      "ak yelpcdn 0.00162228344047\n",
      "lis doon 0.0016083137449\n",
      "khao soi 0.0016048085901\n",
      "malai kofta 0.00160333493667\n",
      "aguas frescas 0.00159387426468\n",
      "porta alba 0.00158899923606\n",
      "rx boiler 0.00157232704403\n",
      "josé andrés 0.00156854543553\n",
      "yadda yadda 0.00156734693878\n",
      "ref 19485 0.00155651340996\n",
      "yada yada 0.00154206302021\n",
      "shiner bock 0.00152623394575\n",
      "buen provecho 0.00151640474221\n",
      "artery clogging 0.00150944337716\n",
      "ritz carlton 0.0015037593985\n",
      "womp womp 0.00149449869876\n",
      "rinky dink 0.0014899576852\n",
      "chino bandido 0.00148665048544\n",
      "sous vide 0.00148544266191\n",
      "homer simpson 0.00147679324895\n",
      "preconceived notions 0.00147106897679\n",
      "harry potter 0.00146881939304\n",
      "hon machi 0.0014679976512\n",
      "jap chae 0.00146705371914\n",
      "auld dubliner 0.00143760782059\n",
      "fra diavolo 0.00143673186775\n",
      "bai thong 0.00143257552164\n",
      "cardiac arrest 0.00142963041406\n",
      "haagen dazs 0.00142493638677\n",
      "bla bla 0.00142026687806\n",
      "val vista 0.00142010153726\n",
      "goi cuon 0.00141187403939\n",
      "scantily clad 0.00141011346494\n",
      "demi glace 0.00140930695258\n",
      "pina colada 0.00140125478618\n",
      "bona fide 0.00139308103088\n",
      "tsk tsk 0.00139204057894\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "q4 = []\n",
    "for a in sorted_dic:\n",
    "    print a, sorted_dic[a]\n",
    "    q4.append(a)\n",
    "    c+=1\n",
    "    if c==100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'ezzyujdouig4p gyb3pv_a',\n",
       " u'hodge podge',\n",
       " u'himal chuli',\n",
       " u'hoity toity',\n",
       " u'roka akor',\n",
       " u'f_5_unx wrafcxuakbzrdw',\n",
       " u'knick knacks',\n",
       " u'reina pepiada',\n",
       " u'cien agaves',\n",
       " u'itty bitty',\n",
       " u'khai hoan',\n",
       " u'riff raff',\n",
       " u'baskin robbins',\n",
       " u'grana padano',\n",
       " u'tutti santi',\n",
       " u'ropa vieja',\n",
       " u'gulab jamun',\n",
       " u'pel meni',\n",
       " u'ore ida',\n",
       " u'laan xang',\n",
       " u'dac biet',\n",
       " u'rula bula',\n",
       " u'hu tieu',\n",
       " u'bandeja paisa',\n",
       " u'tammie coe',\n",
       " u'chicha morada',\n",
       " u'alain ducasse',\n",
       " u'dol sot',\n",
       " u'itsy bitsy',\n",
       " u'mille feuille',\n",
       " u'feng shui',\n",
       " u'hors oeuvres',\n",
       " u'marche bacchus',\n",
       " u'uuu uuu',\n",
       " u'celine dion',\n",
       " u'nanay gloria',\n",
       " u'doon varna',\n",
       " u'luc lac',\n",
       " u'krispy kreme',\n",
       " u'woonam jung',\n",
       " u'perrier jouet',\n",
       " u'deja vu',\n",
       " u'molecular gastronomy',\n",
       " u'puerto rican',\n",
       " u'vice versa',\n",
       " u'patatas bravas',\n",
       " u'sais quoi',\n",
       " u'cullen skink',\n",
       " u'lloyd wright',\n",
       " u'holyrood 9a',\n",
       " u'pura vida',\n",
       " u'lomo saltado',\n",
       " u'nuoc mam',\n",
       " u'wal mart',\n",
       " u'dueling pianos',\n",
       " u'valle luna',\n",
       " u'bradley ogden',\n",
       " u'rustler rooste',\n",
       " u'avant garde',\n",
       " u'honky tonk',\n",
       " u'haricot vert',\n",
       " u'kao tod',\n",
       " u'irn bru',\n",
       " u'ak yelpcdn',\n",
       " u'lis doon',\n",
       " u'khao soi',\n",
       " u'malai kofta',\n",
       " u'aguas frescas',\n",
       " u'porta alba',\n",
       " u'rx boiler',\n",
       " u'jos\\xe9 andr\\xe9s',\n",
       " u'yadda yadda',\n",
       " u'ref 19485',\n",
       " u'yada yada',\n",
       " u'shiner bock',\n",
       " u'buen provecho',\n",
       " u'artery clogging',\n",
       " u'ritz carlton',\n",
       " u'womp womp',\n",
       " u'rinky dink',\n",
       " u'chino bandido',\n",
       " u'sous vide',\n",
       " u'homer simpson',\n",
       " u'preconceived notions',\n",
       " u'harry potter',\n",
       " u'hon machi',\n",
       " u'jap chae',\n",
       " u'auld dubliner',\n",
       " u'fra diavolo',\n",
       " u'bai thong',\n",
       " u'cardiac arrest',\n",
       " u'haagen dazs',\n",
       " u'bla bla',\n",
       " u'val vista',\n",
       " u'goi cuon',\n",
       " u'scantily clad',\n",
       " u'demi glace',\n",
       " u'pina colada',\n",
       " u'bona fide',\n",
       " u'tsk tsk']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score:  0.98\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "score('nlp__food_bigrams', lambda: q4)#[\"kare kare\"] * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*Copyright &copy; 2016 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
