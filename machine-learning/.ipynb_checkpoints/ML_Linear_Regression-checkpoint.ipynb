{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Linear and Logistic Regression\n",
    "<!-- requirement: images/linear_regression_error.gif -->\n",
    "<!-- requirement: images/ridge_lasso.png -->\n",
    "<!-- requirement: small_data/mnist.pkl -->\n",
    "\n",
    "*&copy; The Data Incubator*\n",
    "\n",
    "You should be familiar with linear regression.  Let's talk about it mathematically.\n",
    "\n",
    "## Predictive modeling framework\n",
    "\n",
    "Recall that for this and all other machine-learning setups, $X = \\{X_{ij}\\}$ is an $n \\times p$ matrix of features and $y_i$ is an $n$-vector of labels.  In all **supervised** learning problems, we are trying to build a **model** $f$ (predictive relationship) mapping the feature rows $X_{j \\cdot}$ to each label $y_j$ so that\n",
    "\n",
    "$$f(X_{j \\cdot}) \\approx y_j\\,.$$\n",
    "\n",
    "All supervised learning can be represented in this form.  The parameters that can change are:\n",
    "\n",
    "1. The **model** $f$.  This module will only cover linear models.  We will often assume that one of the columns of $X$ is the constant $1$.  Therefore,\n",
    "\n",
    " $$ f(X_{j \\cdot}) = X_{ji} \\cdot \\beta_i$$\n",
    "\n",
    " represents the entire linear model concisely, including the constant intercept term.\n",
    "\n",
    "1. The **error distribution** of how $y_j$ are distributed.  If the model $f$ is linear, varying the error distribution gives us different classes of the General Linear Models (GLM)s.  We'll talk about the Logistic Regression (arguably the most important GLM after linear regression) but there are many others.  To learn more about GLMs, there are a good set of notes available [here](http://data.princeton.edu/wws509/notes/a2.pdf).\n",
    "\n",
    "With both a model and the error distribution, we can easily compute find the likelihood function.  The problem in GLM is to choose the model that maximizes the likelihood function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Linear Regression\n",
    "\n",
    "Remember the basic picture of linear regression errors:\n",
    "\n",
    "![$L^1$ versus $L^2$ regularization](images/linear_regression_error.gif)\n",
    "\n",
    "Linear Regression is perhaps the simplest linear model $f(X_{j \\cdot}) = \\sum_i \\beta_i X_{ji}$.  The error model assumes the $y_j$'s are independent and normally distributed around $X_{ji} \\cdot \\beta_i$ with standard deviation $\\sigma$.  \n",
    "\n",
    "### Likelihood and cost functions\n",
    "Suppose that we knew that the correct model was given by some $\\beta_i$.  Given the above assumption about the error model, the probability of measuring $y_j$ is simply\n",
    "\n",
    "$$ P(y_j \\mid \\beta_i) = \\prod_j \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp \\left[-\\left( \\frac{X_{ji} \\cdot \\beta_i - y_j}{2 \\sigma} \\right)^2 \\right] \\,.$$\n",
    "\n",
    "However, we don't know the $\\beta_i$.  Instead we want to find them, given the $y_j$, by finding the $\\beta_i$ that maximize $P(\\beta_i \\mid y_j)$.  Thanks to Bayes' Rule, we know\n",
    "\n",
    "$$ P(\\beta_i \\mid y_j) = P(y_j \\mid \\beta_i) \\frac{P(\\beta_i)}{P(y_j)} \\,.$$\n",
    "\n",
    "We know the first term on the RHS, and $P(y_j)$ is independent of $\\beta_i$, leaving only $P(\\beta_i)$ unknown.  In linear regression, we suppose we have no *a priori* knowledge of the expected coeffiecients and take $P(\\beta_i)$ to be constant as well.  Thus, the most probable model is determined by maximizing the likelihood function\n",
    "\n",
    "$$ L(\\beta) = \\prod_j \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp \\left[-\\left( \\frac{X_{ji} \\cdot \\beta_i - y_j}{2 \\sigma} \\right)^2 \\right] \\propto P(\\beta_i \\mid y_j) \\,.$$\n",
    "\n",
    "Since $\\log$ is monotonic, we can also maximize the log-likelihood.  A few calculations show us that the negative log-likelihood (up to a linear transformation) is\n",
    "\n",
    "$$- \\log(L(\\beta)) \\sim \\| y - X \\beta \\|^2\\,.$$\n",
    "\n",
    "Here, $\\| z \\| = \\| z \\|_2 = \\sum_i |z_i|^2 $ is the $L^2$ norm.  The objective is to minimize this quadratic:\n",
    "\n",
    "$$ \\min_\\beta \\| y - X \\beta \\|^2\\,.$$\n",
    "\n",
    "Of course, this is the familiar expression for linear regression.  We could minimize $\\beta$ via gradient descent, but it turns out that the solution has a closed form, \n",
    "\n",
    "$$ X \\hat \\beta = y\\,, $$\n",
    "\n",
    "or\n",
    "\n",
    "$$ \\hat \\beta = (X^T X)^{-1} X^T y\\,. $$\n",
    "\n",
    "** Questions: ** \n",
    "\n",
    "1. What about the intercept term?\n",
    "1. Prove that the solution $\\hat \\beta$ actually minimizes the negative log-likelihood.  (Hint: $X (X^T X)^{-1}X^T$ is the projection operator onto the subsapce spanned by the columns of $X$).\n",
    "1. What happens if $X^T X$ is singular, e.g. $X$ has two columns that are co-linear.  What does this mean in terms of identification?  When might this occur in the data in real life?\n",
    "1. What happens when $p \\gg n$?  How do you deal with this?\n",
    "1. What is the effect of outliers?  How do you deal with them?\n",
    "1. What if $y$ values are always positive?  What if $y$ values are in a fixed range $[a,b]$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Using Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets, linear_model, utils, preprocessing\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import dill\n",
    "\n",
    "with open(\"small_data/mnist.pkl\", \"r\") as fin:\n",
    "    mnist = dill.load(fin)\n",
    "\n",
    "# This dataset gives house prices in Boston based on a variety of factors.\n",
    "# for more information about what the fields mean, checkout http://mldata.org/repository/data/viewslug/regression-datasets-housing/\n",
    "original_columns = [\"CRIM\",\"ZN\",\"INDUS\",\"CHAS\",\"NOX\",\"RM\",\"AGE\",\"DIS\",\"RAD\",\"TAX\",\"PTRATIO\",\"B\",\"LSAT\"]\n",
    "\n",
    "np.random.seed(42)\n",
    "Xraw, y = utils.shuffle(mnist.data, mnist.target)\n",
    "Xraw = pd.DataFrame(Xraw, columns=original_columns)\n",
    "y = pd.Series(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For illustration, we'll start by fitting a single feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X1 = Xraw[['RM']]\n",
    "linreg = linear_model.LinearRegression(fit_intercept=True)  # This is the default\n",
    "linreg.fit(X1, y)\n",
    "plt.plot(X1, y, '.')\n",
    "x = np.linspace(3, 9).reshape(-1,1)\n",
    "plt.plot(x, linreg.predict(x), '-')\n",
    "plt.xlabel('RM')\n",
    "plt.ylabel('Home Price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, let's look at all of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.subplots_adjust(wspace=0.4, hspace=0.6)\n",
    "for i, col in enumerate(Xraw):\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    plt.plot(Xraw[col], y, '.')\n",
    "    plt.locator_params(nbins=6)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Home Price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Some of these features look to be linear, but others aren't.  Somtimes, the inverse of features with negative correlation produces a better fit.  Relationships that look like step functions are better modeled with indicator features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X = Xraw.copy()\n",
    "X[\"INV_CRIM\"] = 1./X.CRIM\n",
    "X[\"INDUS7\"] = X.INDUS <= 7.\n",
    "X[\"INDUS16\"] = 1. * (X.INDUS <= 16.)\n",
    "X[\"INV_NOX\"] = 1./X.NOX\n",
    "X[\"AGE75\"] = 1. * (X.AGE <= 76.)\n",
    "X[\"RAD15\"] = 1. * (X.RAD <= 15.)\n",
    "X[\"TAX500\"] = 1. * (X.TAX <= 500.)\n",
    "X[\"PTRATIO19\"] = X.PTRATIO <= 19.\n",
    "X[\"B350\"] = 1. * (X.B <= 350.)\n",
    "X[\"INV_LSAT\"] = 1. / X.LSAT\n",
    "X=X.astype(float)  # coerce booleans to a float\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's compare the performance of the model on different sets of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from scipy import stats\n",
    "\n",
    "cv = model_selection.ShuffleSplit(n_splits=20, test_size=0.2, random_state=42)\n",
    "def compute_error(est, X, y):\n",
    "    return -model_selection.cross_val_score(est, X, y, cv=cv, scoring='neg_mean_squared_error').mean()\n",
    "\n",
    "def abs_corr(x, y):\n",
    "    return np.abs(stats.pearsonr(x, y)[0])\n",
    "\n",
    "new_columns = ['INV_CRIM', 'ZN', 'INDUS7', 'INDUS16', 'CHAS', 'INV_NOX', \n",
    "               'RM', 'AGE75', 'DIS', 'RAD15', 'TAX500', 'PTRATIO19', 'B350', 'INV_LSAT']\n",
    "\n",
    "model_performance = pd.DataFrame([\n",
    "    (\"Mean Model\", y.var()),\n",
    "    (\"Original Features, Pearson > .6\",\n",
    "       compute_error(linreg, X[[col for col in original_columns if abs_corr(X[col], y) > .6]], y)),\n",
    "    (\"Original Features, Pearson > .4\",\n",
    "       compute_error(linreg, X[[col for col in original_columns if abs_corr(X[col], y) > .4]], y)),\n",
    "    (\"Original Features\", compute_error(linreg, X[original_columns], y)),\n",
    "    (\"New Features\", compute_error(linreg, X[new_columns], y)),\n",
    "    (\"All Features\", compute_error(linreg, X, y)),\n",
    "], columns=[\"Model\", \"MSE\"])\n",
    "model_performance.set_index(\"Model\")\n",
    "model_performance.plot(x=\"Model\", y=\"MSE\", kind=\"Bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "[col for col in original_columns if abs_corr(X[col], y) > .6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### A few notes:\n",
    "\n",
    "1. A good baseline is to see how well a mean model performs, that is, take a model that predicts `y.mean()` and whose MSE is going to be `y.var()`.\n",
    "1. How many (original) features have a correlation coefficient > .6?  These explain the majority of the error (compared with the baseline model).\n",
    "1. One way to prevent this overfitting is to choose only those feautes $X_{\\cdot i}$ that are highly correlated with $y$.  This can lead much better models.\n",
    "\n",
    "**Question:** \n",
    "1. We tried to predict $y$ but since it is non-negative, it might make sense to predict $\\log(y)$.  What metric would you use to be able to evaluate which one is better?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Regularization\n",
    "### Ridge Regression\n",
    "\n",
    "In the problem above, we used an *ad-hoc* criteria to select features.  Essentially, this reflects an expectation that most coefficients should be zero.  A more principled approach is to choose a prior distribution for $P(\\beta_i)$ that is peaked about $\\beta_i = 0$, instead of uniform.  We'll start by taking the coefficients to be identical independently normally distributed about 0 with a standard deviation of $\\sigma/\\sqrt\\alpha$, where $\\alpha$ is a hyperparameter:\n",
    "\n",
    "$$ P (\\beta_i) \\propto \\prod_i \\exp \\left[ -\\frac{\\alpha}{2} \\left(\\frac{\\beta_i}{\\sigma} \\right)^2\\right] \\,,$$\n",
    "\n",
    "so\n",
    "\n",
    "$$ L(\\beta) \\propto \\prod_j \\exp\\left[ -\\frac{1}{2}\\left( \\frac{X_{ji} \\cdot \\beta_i - y_j}{\\sigma} \\right)^2 \\right]\\prod_i \\exp \\left[ -\\frac{\\alpha}{2} \\left(\\frac{\\beta_i}{\\sigma} \\right)^2\\right] \\,.$$\n",
    "\n",
    "Then the negative log-likelihood is (up to a linear transformation)\n",
    "\n",
    "$$- \\log(L(\\beta)) \\sim \\| y - X \\beta \\|^2 + \\alpha \\| \\beta \\|^2\\,.$$\n",
    "\n",
    "After compleing the square, it turns out that the solution has a closed form, \n",
    "\n",
    "$$ \\hat \\beta = (X^T X + \\alpha I)^{-1} X^T y\\,. $$\n",
    "\n",
    "To get some motivation for what's happening, use the *singular value decomposition*\n",
    "\n",
    "$$ X = U \\Sigma V^T $$\n",
    "\n",
    "We  can see that \n",
    "\n",
    "$$ \\hat \\beta = V D U^T y $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ D_{ii} = \\frac{\\Sigma_{ii} }{\\Sigma_{ii}^2 + \\alpha}\\,. $$\n",
    "\n",
    "When $\\alpha = 0$, $D_{ii} = \\frac{1}{\\Sigma_{ii}}$ and it decreases to 0 as $\\alpha \\to \\infty$.  The smaller $\\Sigma_{ii}$, the faster this decrease to 0 (for a given level of $\\alpha$).  So smaller $\\Sigma_{ii}$ are \"shrunk\" faster than larger $\\Sigma_{ii}$ and we get the \"significant values\" are left.\n",
    "\n",
    "**Questions:**\n",
    "1. Are \"significant values\" always left?  What pre-processing step might one do to ensure this?\n",
    "1. Can you prove the formula for $\\hat \\beta$ for Ridge Regression from ordinary Linear Regression?\n",
    "1. What is the corresponding prior for plain-vanilla linear regression?\n",
    "1. Compared with linear regression, how to you expect the $\\beta$'s to behave?  How does this behavior change as you vary $\\alpha$?\n",
    "\n",
    "This technique is called many things including **Shrinkage**, **$L^2$-regularization**, ** Tikhonov-Regularization**, **Ridge-regression**.  We can use cross-validation to choose an optimal value of $\\alpha$.  The answer is still quadratic in $\\beta$ so we can compute the answer in closed form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Ridge regression using cross validation linear_model.Ridge\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "alphas = np.logspace(-4., 0, 20)\n",
    "ridge_models = pd.DataFrame(\n",
    "    [(alpha,\n",
    "      \"Ridge Regression with alpha = %f\" % alpha, \n",
    "      compute_error(linear_model.Ridge(alpha=alpha), X, y)) for alpha in alphas],\n",
    "    columns=['alpha', 'Model', 'MSE'])\n",
    "ridge_models.plot(x='alpha', y='MSE', logx=True, title='MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "coefs = np.array([linear_model.Ridge(alpha=alpha).fit(X, y).coef_\n",
    "                  for alpha in alphas])\n",
    "plt.semilogx(alphas, coefs)\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('Coefficients')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Lasso Regression\n",
    "\n",
    "Lasso is like ridge regression but has the ability to automatically select features.  The objective function to minimize is\n",
    "\n",
    "$$ \\frac{1}{2 n} \\| y - X^T \\beta \\|^2 + \\alpha \\|\\beta\\|_1 $$\n",
    "\n",
    "where $\\|\\beta\\|_1 = \\sum_i |\\beta_i| $ is the $L^1$ norm (sum of the absolute values) of $\\beta$ and $n$ is the number of samples.  This is called **Lasso Regression** or **$L^1$-Regularization** because it is basically Ridge Regression where the extra prior term is an $L^1$ penalty instead of an $L^2$ penalty.  Lasso has a feature selection property where many weights on features are zero (i.e. those features are not selected).\n",
    "\n",
    "We can see this behavior by contrasting the effects of regularization on a single parameter.  Because the $L^2$ regularization has slope 0 at $\\beta=0$, the minimal value of $\\beta$ will never reach 0 for finite $\\alpha$.  This is not the case for lasso regression, where a finite $\\alpha$ can make the optimal $\\beta$ go all the way to zero, excluding that feature from consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "beta = np.linspace(-1,2,100)\n",
    "base_error = (beta - 1)**2 - 1\n",
    "\n",
    "for pow in [2,1]:\n",
    "    plt.subplot(123 - pow)\n",
    "    for a in xrange(0, 5):\n",
    "        plt.plot(beta, base_error + a * abs(beta)**pow, label=r'$\\alpha = %i$' % a)\n",
    "    plt.ylim(-2, 4)\n",
    "    plt.xlabel(r'$\\beta$')\n",
    "    plt.ylabel(r'$-\\log(L(\\beta))$')\n",
    "    plt.legend(loc=3)\n",
    "    plt.title({2: 'Ridge Regression', 1: 'Lasso Regression'}[pow])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Lasso Regression with cross validation\n",
    "np.random.seed(42)\n",
    "alphas = np.logspace(-5., -1., 20)\n",
    "\n",
    "lasso_models = pd.DataFrame(\n",
    "    [(alpha,\n",
    "      \"Lasso with alpha = %f\" % alpha,\n",
    "      compute_error(linear_model.LassoLars(alpha=alpha), X, y)) for alpha in alphas],\n",
    "    columns=['alpha', 'Model', 'MSE'])\n",
    "lasso_models.plot(x='alpha', y='MSE', logx=True, title='MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "coefs = np.array([linear_model.LassoLars(alpha=alpha).fit(X, y).coef_\n",
    "                  for alpha in alphas])\n",
    "plt.semilogx(alphas, coefs)\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('Coefficients')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can also build our intuition of Ridge and Lasso regression by considering the figure below. Consider two features whose weights $\\beta_1$ and $\\beta_2$ are represented by the $x$ and $y$ axes, respectively. The error function is shown in grey and its minimum is located at the center of the ellipses, at $\\vec{\\beta} = \\hat{\\beta}$. However, given the red constraint regions, $\\beta_1^2 + \\beta_2^2 \\leq \\alpha^2$ (left) and $|\\beta_1| + |\\beta_2| \\leq \\alpha$ (right), the minima of error functions now lie on the edges of the constraint regions. Since the $L^1$ constraint forms a diamond, new values for $\\hat{\\beta}$ will tend to fall on the corners where 1 component (or more than one component) of $\\beta$ zero. For this reason Lasso regresson \"throws out\" features while Ridge regresson decreases the weights or contributions of features without making them zero.\n",
    "![$L1$ versus $L2$ regularization](images/ridge_lasso.png)\n",
    "\n",
    "### Elastic Net\n",
    "\n",
    "You can combine both $L^1$ and $L^2$ regularization in something called Elastic Net.\n",
    "\n",
    "### Cross-validation and warm starts\n",
    "\n",
    "There's no closed-form solution for either Lasso or Elastic net so they required a numerical solution, e.g. gradient descent.  Take a look at the functions `linear_model.lasso_path` and `linear_model.enet_path`, which use a warm start technique to speed up the cross-validation.  Also note that there is `linear_model.RidgeCV`, `linear_model.LassoCV`, which return the best estimators using warm-starts.  Note that the default cross-validation is the slow leave-one-out algorithm so you'll probably want to tell it to use something like KFold cross-validation:\n",
    "``` python\n",
    "linear_model.RidgeCV(\n",
    "    alphas=np.logspace(-1, 1, 5),\n",
    "    cv=cross_validation.KFold(5))\n",
    "```\n",
    "\n",
    "**Questions:** \n",
    "\n",
    "1. In $L^2$ regularization, the prior on $\\beta$ are i.i.d normal distributions centered around 0.  What is the analogous prior for $L^1$ regularization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Exercises**: \n",
    "* Use `lasso_path` to rewrite the inefficient cross validation done below and reproduce the same plot.  Does it run faster?\n",
    "* Normalize the feature matrix before fitting with $L^1$ and $L^2$-regularized regressions.  Examine the coefficients of the fit.  Does the normalization change which features appear to be most important?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Combining signals with linear models\n",
    "\n",
    "One of the great uses of linear regression (or any regression / classificaiton technique) is to combine different machine-learned signals.\n",
    "\n",
    "For example, we can try to use non-parametric techniques on the residual error from a linear classifier.  Since the linear classifier has found all the 'linear signals', this will find some of the remaining 'non-linear signals'.\n",
    "\n",
    "Below is an example of a model that does this.  By making this a Scikit-Learn estimator (by virtue of being a pipeline), we are able to take advantage of the existing cross-validation infrastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, TransformerMixin\n",
    "from sklearn import neighbors\n",
    "from sklearn import ensemble\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "\n",
    "class EnsembleTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, base_estimator, residual_estimators):\n",
    "        self.base_estimator = base_estimator\n",
    "        self.residual_estimators = residual_estimators\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.base_estimator.fit(X, y)\n",
    "        y_err = y - self.base_estimator.predict(X)\n",
    "        for est in self.residual_estimators:\n",
    "            est.fit(X, y_err)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        all_ests = [self.base_estimator] + list(self.residual_estimators)\n",
    "        return np.array([est.predict(X) for est in all_ests]).T\n",
    "\n",
    "\n",
    "ensemble_pipe = Pipeline([\n",
    "        ('ensemble', EnsembleTransformer(\n",
    "                linear_model.LinearRegression(),\n",
    "                (neighbors.KNeighborsRegressor(n_neighbors=5),\n",
    "                 ensemble.RandomForestRegressor(min_samples_leaf=20)))),\n",
    "        ('blend', linear_model.LinearRegression())\n",
    "    ])\n",
    "ensemble_pipe.fit(X, y)\n",
    "    \n",
    "residual_regressor_performance = pd.DataFrame([\n",
    "    (\"Ensemble Regressor\", compute_error(ensemble_pipe, X, y))\n",
    "], columns=[\"Model\", \"MSE\"])\n",
    "model_performance.append(residual_regressor_performance).plot(x=\"Model\", y=\"MSE\", kind=\"Bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Exercises:** \n",
    "\n",
    "1. Add another regression technique into the mix (besides the 3 used here).  Does it improve performance?\n",
    "1. The values of `n_neighbors` and `min_samples_leaf` were set at random.  Use cross validation to select the value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "In Logistic Regression, the values of $y$ are categorical ($0$ or $1$) and assumed to be distributed binomially.  We assume that the probability $p(X_{j\\cdot})$ that $y = 1$ is related to $X$ via the logit function:\n",
    "\n",
    "$$ \\mbox{logit }(p(X_{j\\cdot})) = \\log \\frac{p(X_{j\\cdot})}{1-p(X_{j\\cdot})} = X_{ji} \\cdot \\beta_i\\,. $$\n",
    "\n",
    "Notice that the logit function $\\log \\frac{x}{1-x}$ is just the log odds and maps the real numbers $[0,1]$ to $\\mathbb{R}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lx = np.linspace(0.01,0.99,98)\n",
    "ly = np.log(lx/(1-lx))\n",
    "plt.plot(lx,ly)\n",
    "plt.xlabel('$p$')\n",
    "plt.ylabel(r'$\\mathrm{logit}(p)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "It may be more clear to invert this to get an expression for $p(X_{j\\cdot})$:\n",
    "\n",
    "$$ p(X_{j\\cdot}) = \\frac{\\exp\\left( X_{ji} \\cdot \\beta_i\\right)}{1 + \\exp\\left( X_{ji} \\cdot \\beta_i\\right)} $$\n",
    "\n",
    "The input can vary over the entire real numbers, but the output is always a valid probability between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lx = np.linspace(-10, 10)\n",
    "ly = np.exp(lx) / (1 + np.exp(lx))\n",
    "plt.plot(lx, ly)\n",
    "plt.xlabel(r'$X_{ji} \\cdot \\beta_i$')\n",
    "plt.ylabel(r'$P(X_{j\\cdot})$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The full likelihood function is given by\n",
    "\n",
    "$$ L(\\beta) = \\prod_j p(X_{j\\cdot})^{y_j} (1-p(X_{j\\cdot}))^{1-y_j}\\,. $$\n",
    "\n",
    "Notice that the log-likelihood is just \n",
    "$$ \\log(L(\\beta)) = \\sum_j y_j \\log(p(X_{j\\cdot})) + (1-y_j) \\log(1-p(X_{j\\cdot})) $$\n",
    "Since the objective is to maxmize $L$, we can use gradient descent (on $\\log(L)$) to compute the answer,\n",
    "\n",
    "$$ \\frac{\\partial \\log(L(\\beta))}{\\partial \\beta} = \\sum_j (y_j - p(X_{j\\cdot})) X_{j\\cdot} $$\n",
    "\n",
    "The gradient is quite intuitive.  The derivation is worked out [here](http://cs229.stanford.edu/notes/cs229-notes1.pdf).\n",
    "\n",
    "In the following example, we'll try to predict whether the home price is greater than or less than $25K.\n",
    "\n",
    "**Questions:** \n",
    "1. What is the negative log-likelihood that is being minimized?\n",
    "1. The Scikit Learn library already contains $L^1$ and $L^2$ regularization built in.  Can you write down the minimization problem with the $L^1$ and $L^2$ penalties?\n",
    "1. There's a `weight` parameter to Scikit's `LogisticRegression` that lets you reweight different training examples.  When might you want to do this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "# Sklearn's logistic regression comes with ROC-AUC as our objective\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "y_cat = y > 25.\n",
    "logistic_clf = linear_model.LogisticRegression()\n",
    "\n",
    "Cs = np.logspace(-2,2,22)\n",
    "lasso_models = pd.DataFrame(\n",
    "    [(C,  # Inverse regularization parameter\n",
    "      model_selection.cross_val_score(linear_model.LogisticRegression(C=C), \n",
    "                                       X, y_cat, cv=cv, scoring='roc_auc').mean()\n",
    "    ) for C in Cs],\n",
    "    columns=['C', 'ROC_AUC'])\n",
    "lasso_models.plot(x='C', y='ROC_AUC', logx=True, title='ROC_AUC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Multiclass classification problems\n",
    "\n",
    "So far we have talked about Two-Class classification in the context of Logistic Regression.  But what if we have more than two classes?  There are generally two strategies to \"bootstrap\" a binary classifier to a multi-class classifier: \n",
    "1. **One-versus-All**: For each class $k=1,\\ldots,K$, build a binary classifier for all points with label $y = k$ versus $y \\neq k$.\n",
    "1. **All-versus-All**: For each class $k \\neq k'$, construct a binary classifier to distinguish between class $k$ and $k'$.\n",
    "There's also the notion of Error-Correcting Output Codes \n",
    "\n",
    "[Scikit](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) uses One-versus-All for Multi-class Logistic Regression.  If $f_k(x)$ is the predictor for class $k$, the probability of class $k$ is just the normalized predicitons,\n",
    "\n",
    "$$ p_k = \\frac{f_k(x)}{\\sum_k f_k(x)}$$\n",
    "\n",
    "Scikit provides a way to do other multiclass-from-binary-classifier strategies in [Scikit-Documentation](http://scikit-learn.org/stable/modules/multiclass.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Minimizing $L^1$ rather than $L^2$ error in a regression.\n",
    "\n",
    "In traditional linear regression the penalty is quadratic, which unfortuantely overpenalizes for outliers.  We often want to use absolute error instead.  There are a few ways to do this:\n",
    "1. The first is to use SVM Regression `sklearn.svm.SVR`.  For `SVR`, the penalty function is the function\n",
    "\n",
    " $$ \\max(|x| - \\epsilon, 0) $$\n",
    "\n",
    "1. The stochastic gradient descent regressor (`sklearn.linear_model.SGDRegressor`) offers the [Huber loss function](http://en.wikipedia.org/wiki/Huber_loss_function),\n",
    "\n",
    "$$ \\frac{1}{2} x^2 I_{|x| \\le \\delta} + \\delta\\left(|x| - \\frac{\\delta}{2}\\right)  I_{|x| > \\delta}$$\n",
    "\n",
    "We plot out a comparison of these methods below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = np.arange(-4,4,.1)\n",
    "y_quad = x ** 2/2\n",
    "y_svr = np.maximum(np.abs(x)-1.,0)\n",
    "y_huber = (x ** 2/2) * (np.abs(x) <= 1) + (np.abs(x) - .5) * (np.abs(x) > 1)\n",
    "\n",
    "plt.title(\"Penalty functions\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.plot(x, y_quad, label=\"MSE\")\n",
    "plt.plot(x, y_svr, label=\"SVR\")\n",
    "plt.plot(x, y_huber, label=\"Huber\")\n",
    "plt.legend(loc='upper center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "Stochastic Gradient Descent is a optimization method which considers each training data individually, instead of all at once.  It uses each datum to estimate the gradient of the penalty function and then takes a step in that direction.  While each individual point will provide a poor estimation of the global minimum, combining all of these estimates results in a good global estimation.\n",
    "\n",
    "Because it need only consider a single datum at a time, stochastic gradient descent can handle data sets too large to fit in memory.  Additionally, the training cost is essentially linear in the size of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Xt = preprocessing.StandardScaler().fit_transform(X)\n",
    "\n",
    "coefs = []\n",
    "iterations = range(1,5000,100)\n",
    "for n_iter in iterations:\n",
    "    sgd_regressor = linear_model.SGDRegressor(random_state=42, n_iter=n_iter).fit(Xt, y)\n",
    "    coefs.append(sgd_regressor.coef_)\n",
    "\n",
    "    \n",
    "dists = [np.linalg.norm(coef - coefs[-1]) for coef in coefs]\n",
    "plt.figure()\n",
    "plt.plot(iterations, dists)\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('distance from last')\n",
    "\n",
    "plt.figure()\n",
    "for i in xrange(4):\n",
    "    plt.plot(iterations, [c[i] for c in coefs], label='coefs[%i]' % i)\n",
    "plt.legend(loc=3)\n",
    "plt.xlabel('iteration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Quantile Regression\n",
    "\n",
    "In a linear regression, the $L^2$ penalty in the error function means the algorithm is very sensitive to outliers. (**Question**: can you see why?)  One way to control features that have fat tails is to take a quantile transformation of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# find the column with the highest L^2 moment compared to L^1 moment: this happens to be DIS\n",
    "\n",
    "l2_to_l1 = np.array([np.sqrt(X[col].var()) /\n",
    "                     (X[col] - X[col].mean()).abs().mean() for col in original_columns])\n",
    "max_col_idx = np.argmax(l2_to_l1)\n",
    "\n",
    "max_col_idx = 7\n",
    "print pd.DataFrame([\n",
    "    (\"Col\", X.columns[max_col_idx]),\n",
    "    (\"Max Ratio\", l2_to_l1[max_col_idx]),\n",
    "    (\"Average Ratio\", l2_to_l1.mean())\n",
    "], columns=[\"Model\", \"MSE\"])\n",
    "\n",
    "# create a new list of column names with this column as _Quantile\n",
    "new_columns = list(original_columns)\n",
    "new_columns[max_col_idx] = original_columns[max_col_idx] + \"_Quantile\"\n",
    "\n",
    "# sort data in this column to compute the \"quantilized\" value\n",
    "def to_quantile(data):\n",
    "    sorted_data = sorted(data)\n",
    "    return np.array([1. * sorted_data.index(d)  / len(data) for d in data])\n",
    "X[new_columns[max_col_idx]] = to_quantile(X[X.columns[max_col_idx]])\n",
    "\n",
    "compute_error(linear_model.LinearRegression(), X[new_columns], y)\n",
    "\n",
    "# Compare results\n",
    "pd.DataFrame([\n",
    "    (\"Original Columns\", compute_error(linear_model.LinearRegression(), X[original_columns], y)),\n",
    "    (\"With a new Quantile Column\", compute_error(linear_model.LinearRegression(), X[new_columns], y))\n",
    "], columns=[\"Model\", \"MSE\"]).plot(x=\"Model\", y=\"MSE\", kind=\"Bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Let's see how close \n",
    "\n",
    "def compute_regression_with_quatilized_feature(max_col_idx):\n",
    "    # create a new list of column names with this column as _Quantile\n",
    "    new_columns = list(original_columns)\n",
    "    new_columns[max_col_idx] = original_columns[max_col_idx] + \"_Quantile\"\n",
    "\n",
    "    # sort data in this column to compute the \"quantilized\" value\n",
    "    def to_quantile(data):\n",
    "        sorted_data = sorted(data)\n",
    "        return np.array([1. * sorted_data.index(d)  / len(data) for d in data])\n",
    "    X[new_columns[max_col_idx]] = to_quantile(X[X.columns[max_col_idx]])\n",
    "\n",
    "    return compute_error(linear_model.LinearRegression(), X[new_columns], y)\n",
    "\n",
    "# Assemble results and plot\n",
    "quantilized_models = pd.DataFrame([(\"Original\", compute_error(linear_model.LinearRegression(), X[original_columns], y))] + \n",
    "             [(original_columns[i],\n",
    "               compute_regression_with_quatilized_feature(i))\n",
    "            for i in range(len(original_columns))],\n",
    "columns=[\"Col\", \"Quantile\"])\n",
    "quantilized_models.plot(x=\"Col\", y=\"Quantile\", kind=\"Bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### GLM\n",
    "\n",
    "TODO: add in the future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Questions\n",
    "1. How would you assess whether a relationship is actually linear?\n",
    "1. If instead of being able to observe $y$, you observe a noisy estimate of $y \\pm \\epsilon$ with unbiased normally distributed noise.  What is the effect on your estimates $\\beta$?\n",
    "1. When you loaded your data, you unwittingly loaded each row of the data (both $X$ and $y$) twice and performed the same regression.  What is the effect on your estimates $\\beta$?\n",
    "1. When you loaded your data, you unwittingly loaded each column of the features (just $X$) twice and performed the same regression.  What is the effect on your estimates $\\beta$?\n",
    "1. Everything we've talked about so far involves loading all the data into memory.  What if you have more data than you can fit into memory?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Exit Tickets\n",
    "1. Enumerate the similarities and differences between linear regression and logistic regression.\n",
    "1. Explain to a layman what a likelihood function or cost function is.\n",
    "1. Compare Ridge and Lasso regression (L2/L1 regularization) in terms of how they affect variance-bias.\n",
    "1. What are the benefits/drawbacks to optimizing the likelihood function iteratively versus all at once?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Spoilers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Answers\n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "1. Add a column of ones, or subtract off the average value of $y$.\n",
    "1. The hint pretty much gives you the answer since a projection of a point onto a plane is the closest you can get to that point while still remaining on the plane. That being said, you can project the global minimum onto the accessible subspace.  Alternatively, you can differentiate the matrix expression and take the first-order condition and find the zero.\n",
    "1. The matrix can't be inverted.  In practice, this shows up as numerical instabilities.  This will happen if two columns are measuring the same thing, or if one column is a linear combination of two others.\n",
    "1. When $p > n$, X will be degenerate, so it can't be (psuedo-)inverted and you no longer have a unique $\\beta$.  To deal with this, you can reduce the number of features with PCA or use regularization.\n",
    "1. Outliers can really skew the results of $\\beta$ because of the quadratic penalty.  Remember, that minimizing the least squares is essentially looking for a mean, which is affected by outliers.  You can transform the model via quantiles to reduce the effect of noise, bin the data, or use floors and caps on the data.\n",
    "1. For non-negative $y$, try using the $\\log(y)$.  If $y$ is always within a fixed $[a,b]$, use $$\\frac{y - a}{b-a}\\,.$$ Alternatively, scale by the mean / range. \n",
    "\n",
    "### Using Linear Regression\n",
    "\n",
    "1. Look at coefficient of determination; plot residuals and look for a pattern.\n",
    "\n",
    "### Ridge Regression\n",
    "\n",
    "1. No, just the largest coefficients are left.  Scale the features beforehand, so that coefficient size gives importance.\n",
    "1. The formula for $\\hat \\beta$ can be deduced by completing the square.  Then the problem looks exactly like an ordinary Least Squares problem with a different $X$ matrix.\n",
    "1. Recall that the prior is\n",
    "$$\\exp \\left[ -\\left( \\alpha \\frac{\\beta}{2 \\sigma} \\right)^2\\right]$$\n",
    "when $\\alpha = 0$, this is a flat \"improper\" prior (it's not really a distribution).  This is often what a bayesian calls improper.\n",
    "1. Increasing $\\alpha$ shrinks the terms of $\\beta$ towards zero, with smaller values of $\\beta$ shrunk faster.\n",
    "\n",
    "### Lasso Regression\n",
    "\n",
    "1. The prior is a two-sided exponential distribution.\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "1.  The negative Log-likelihood (entropy) is\n",
    "\n",
    "    $$- \\log(L(\\beta)) = - \\sum_j y_j \\log(f(X_{j\\cdot})) + y_j \\log(1-f(X_{j\\cdot}))$$\n",
    "\n",
    "1.  Just add regularization terms to the log loss. The $L^2$ constraint is\n",
    "\n",
    "    $$- \\sum_j y_j \\log(f(X_{j\\cdot} \\cdot \\beta)) + y_j \\log(1-f(X_{j\\cdot} \\cdot \\beta)) + \\alpha \\| \\beta \\|^2_2$$\n",
    "    \n",
    "1. One answer is if one class is very common (e.g. 99% of the data), you can downsample it it and then use the weight to give an unbiased estimate.  A classic example is click-prediction in advertising, where clicks are rare compared to non-clicks.\n",
    "\n",
    "### Module Questions\n",
    "\n",
    "1. To assess if the relationship is linear, plot the distribution of the residuals as a function of $x$.  If there's a systematic bias, take a look at it and see what's going on.\n",
    "1. With extra (unbiased) noise, the estimate of $\\beta$ does not change (on average), but the the confidence goes down.\n",
    "1. Loading rows twice has no effect on $\\beta$ but it does artificially increase your confidence (dividing it by a factor $\\sqrt{2}$)\n",
    "1. The problem becomes degenerate and $\\beta_j$ is now split between $\\beta_{j'}$ and $\\beta_{j''}$ such that $\\beta_j = \\beta_{j'} + \\beta_{j''}$.\n",
    "1. All of these problems can be solved using gradient descent, which only requires a *stream* of data, rather than the entire dataset.  Linear regression (with either $L^2$, Huber penalty, epsilon insensitive) can be solved using `sklearn.linear_model.SGDRegressor` and logistic regression can be solved using `sklearn.linear_model.SGDClassifier`.  These methods implement a `partial_fit` method, which can iteratively updates the coefficients on small chunks of data.  In this case, you are no longer ram constrained, but constrained in the amount of time it takes to read data from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X = np.random.randn(24).reshape([6,4])\n",
    "y = np.random.randn(6)\n",
    "\n",
    "# linear regression, the solution is overdetermined but \n",
    "# scikit uses's scipy's SVD algorithm which is robust to this.\n",
    "X_two_col = np.hstack([X, X[:,-1:]])\n",
    "clf = linear_model.LinearRegression().fit(X,y)\n",
    "clf.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Exit Tickets\n",
    "\n",
    "1. Both multiplies the features by a weight vector.  Linear regression just uses those values; logistic regression maps into the range [0, 1].\n",
    "1. It measures the probability of various models, given the observed data.\n",
    "1. Lasso will completely exclude features, so the bias will arrive in discrete chunks.\n",
    "1. \n",
    "\n",
    "Benefits:\n",
    "* Speed -- you can split the inputs and parallelize the process.\n",
    "* Memory -- you can iteratively train on as much data that fits into memory.\n",
    "* Streaming -- you can update/improve your model with additional data.\n",
    "\n",
    "Cons:\n",
    "* Low final accuracy -- the gradient is susceptible to noise.\n",
    "* Reproducibility. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*Copyright &copy; 2015 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
