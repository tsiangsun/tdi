{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<!-- requirement: images/matrix.svg -->\n",
    "\n",
    "# Learning and Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Supervised machine learning\n",
    "\n",
    "For this and all other machine-learning setups, \n",
    "\n",
    "$$X = \\{X_{ji}\\}$$\n",
    "\n",
    "is an $n \\times p$ matrix of features ($1 \\le j \\le n$ and $1 \\le i \\le p$) and \n",
    "\n",
    "$$y_j$$\n",
    "\n",
    "is an $n$-vector of labels.  In all **supervised** learning problems, we are trying to build a **model** $f$ (predictive relationship) that maps the feature rows $\\{X_{j \\cdot}\\}$ to each label $y_j$ so that \n",
    "\n",
    "$$f(X_{j \\cdot}) \\approx y_j.$$\n",
    "\n",
    "With this predictive model, we will be able to predict the label associated with a new feature row $\\tilde X_{j \\cdot}$ via $f(\\tilde X_{j \\cdot})$.  That's it.\n",
    "\n",
    "![Feature matrix](images/matrix.svg)\n",
    "\n",
    "A few key concepts:\n",
    "1. Within Supervised machine learning, there are roughly two broad classes of problems, **Regression** and **Classification**.  Regression is when the values of $y$ are continuous and real-valued.  Classification is when $y$ takes on a discrete discrete number of possible values.\n",
    "1. What does it mean for the prediction to be accurate?  That depends on what **metric** we use, which is at the discretion of the modeler.  Obviously, some metrics make sense for regression, and others for classification.\n",
    "\n",
    "**Question:** \n",
    "1. What are some examples of classification versus regression problems or algorithms?\n",
    "1. These are all examples of supervised learning problems or algorithms.  Do you know of an example of an unsupervised learning problem or algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Metrics for regression\n",
    "\n",
    "**Sum of Squared Error** is the usual metric:\n",
    "\n",
    "$$ \\sum_j \\left[f(X_{j \\cdot}) - y_j\\right]^2. $$\n",
    "\n",
    "Unfortunately, this is susceptible to outliers. When this is an issue, **Absolute Error** can be better:\n",
    "\n",
    "$$ \\sum_j \\left|f(X_{j \\cdot}) - y_j\\right|. $$\n",
    "\n",
    "You've probably heard of **$R^2$** or the **Coefficient of Determination**. Although it's usually defined in a linear regression context, it's actually a very general idea: it measure the fraction of the error explained by the model $f$ versus the fraction of the error explained by a naive model that assumes the mean value of $y$ (i.e. the variance of $y$):\n",
    "\n",
    "$$ 1 - \\dfrac{\\sum_j \\left[f(X_{j \\cdot}) - y_j\\right]^2}{\\sum_j \\left(\\overline y - y_j\\right)^2} \\qquad \\mbox{where} \\qquad \\overline y = \\frac{1}{n}\\sum_j y_j \\,.$$\n",
    "\n",
    "**Questions:**\n",
    "\n",
    "1. For a list of scalar values $z_1,\\ldots,z_n$, the **mean** $\\overline z$ is the quantity that minimizes the squared error:\n",
    " $$ \\frac{d}{dz} \\sum_j \\left|z - z_j\\right|^2 = 0$$\n",
    " $$ \\frac{d}{dz} \\left(Nz^2 - 2z(z_1 + z_2 + ...) + z_1^2 + z_2^2 + ...\\right) = 0$$\n",
    " $$ 2Nz - 2(z_1 + z_2 + z_3 + ...) = 0$$\n",
    "\n",
    " $$ z = \\frac{z_1 + z_2 + z_3 + ...}{N} = \\overline z$$\n",
    "\n",
    " Do you know what quantity comes from minimizing the absolute error?\n",
    " $$ \\mbox{argmin}_z \\sum_j \\left|z - z_j\\right| $$\n",
    " Does this help explain why Absolute Error is less susceptible to outliers?\n",
    "1. How does each of these metrics scale as you scale the labels ($y$'s) in our data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.0901900155415\n",
      "Mean Squared Error: 0.0113860437222\n",
      "R^2: 0.993949604835\n"
     ]
    }
   ],
   "source": [
    "# Here are those metrics in scikit learn\n",
    "\n",
    "from sklearn import metrics\n",
    "from numpy import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "y_obs  = 2*random.randn(10)\n",
    "y_pred = y_obs + .1 * random.randn(10)\n",
    "\n",
    "print \"Mean Absolute Error:\", metrics.mean_absolute_error(y_obs, y_pred)\n",
    "print \"Mean Squared Error:\", metrics.mean_squared_error(y_obs, y_pred)\n",
    "print \"R^2:\", metrics.r2_score(y_obs, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Metrics for classification\n",
    "\n",
    "There are a plethora of metrics for classification and they depend on whether the predictions are given in terms of the potential label classes or probabilities.\n",
    "\n",
    "Let's start with the simplest.\n",
    "\n",
    "Recall this well-known table\n",
    "\n",
    "|                     | Observation Positive     | Observation Negative    |\n",
    "|---------------------|:------------------------:|:-----------------------:|\n",
    "| Prediction Positive |     True Positive        | False Positive (Type I) |\n",
    "| Prediction Negative | False Negative (Type II) |     True Negative       |\n",
    "\n",
    "There are many summary statistics one can compute from this table:\n",
    "1. The **Accuracy** gives the fraction labels correctly predicted (True Positives and True Negatives over everything).  \n",
    "1. The **Hamming Loss** gives the fraction of labels incorrectly predicted.  It is 1 - Accuracy.\n",
    "1. The **Precision** is true positives divided by all positive *predictions*\n",
    "1. The **Recall** is true positives divided by all positive *observations*.\n",
    "1. There is also **F-beta** score which gives a weighted geometric average between the precision and recall (as a function of $\\beta$) and the **F-1** score is the special case when $\\beta = 1$.\n",
    "1. The **Jaccard Similarity Coefficient** is in general the intersection of the predicted and actual label set divided by the union. This is equivalent to the accuracy score for most classification problems.\n",
    "\n",
    "**Questions:**\n",
    "1. What's the interpretation of precision and recall?\n",
    "1. When would you want high precision? High recall?\n",
    "1. Is Harvard's admission's process high precision or high recall?  \n",
    "1. What about Sir Blackstone's aphorism \"Better that ten guilty persons escape than that one innocent suffer\" with Captain Louis Renault's order to \"Round up the Usual Suspects\" in the film \"Casablanca\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Accuracy and Hamming distnace:\n",
    "\n",
    "y_obs  = [0, 0, 1, 1, 0, 1, 0, 1]\n",
    "y_pred = [0, 0, 1, 1, 0, 0, 0, 1]\n",
    "\n",
    "print \"Accuracy:\", metrics.accuracy_score(y_obs, y_pred)\n",
    "print \"Hamming Loss:\", metrics.hamming_loss(y_obs, y_pred)\n",
    "print \"Precision:\", metrics.precision_score(y_obs, y_pred)\n",
    "print \"Recall:\", metrics.recall_score(y_obs, y_pred)\n",
    "print \"F1:\", metrics.f1_score(y_obs, y_pred)\n",
    "print \"Jaccard:\", metrics.jaccard_similarity_score(y_obs, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# summary report\n",
    "# http://scikit-learn.org/stable/modules/model_evaluation.html#classification-report\n",
    "from sklearn.metrics import classification_report\n",
    "print classification_report(y_obs, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Metrics for probabilistic predictions\n",
    "\n",
    "### Precision-recall tradeoff\n",
    "\n",
    "When the predictions are in terms of probabilities, we have a different set of metrics.  Then the above metrics are not well defined.  However, there is an implicit tradeoff between the two.\n",
    "\n",
    "Let us take the two-class example where each $y_j$ is either positive (1) or negative (0) and we make a probabilistic prediction $p_j$.  A reasonable solution would be to choose a **threshold** $\\underline p$ such that $p_j > \\underline p$ is a Positive Prediction and $p_j \\le \\underline p$ is a negative one.  Hence, for every choice of $p_j$, we can compute a precision and a recall.  Varying $\\underline p$ creates a family (or curve) of Precision Recall pairs.\n",
    "\n",
    "**Questions (Part 1):** \n",
    "1. How does increasing $\\underline p$ affect the Precision or Recall?  What assumption do you have to make in order to answer this?\n",
    "1. How does the curve vary depending on the quality of the estimator $f$ that produces the predictions $p_j$.  What if the estimator were perfect?  What if it were guessing at random?\n",
    "1. What if $f$ were a reasonably good estimator but you used $1-f$ as your estimator?\n",
    "1. How do you decide how to make the tradeoff between precision and recall?  How does this relate to the cost of a false positive versus false negative?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "x = np.r_[0:1:1000j]\n",
    "y = random.binomial(1, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "def plot_threshold(threshold=0.5):\n",
    "    true_pos = (x > threshold) * (y > 0)\n",
    "    plt.plot(x[true_pos], y[true_pos], '.', label=\"True Positive\")\n",
    "    false_pos = (x > threshold) * (y == 0)\n",
    "    plt.plot(x[false_pos], y[false_pos], '.', label=\"False Positive\")\n",
    "    true_neg = (x <= threshold) * (y == 0)\n",
    "    plt.plot(x[true_neg], y[true_neg], '.', label=\"True Negative\")\n",
    "    false_neg = (x <= threshold) * (y > 0)\n",
    "    plt.plot(x[false_neg], y[false_neg], '.', label=\"False Negative\")\n",
    "    plt.axvline(threshold, c='k')\n",
    "    plt.ylim(-0.5, 1.5)\n",
    "    plt.legend()\n",
    "    \n",
    "    try:\n",
    "        precision = 1.0 * sum(true_pos) / (sum(true_pos) + sum(false_pos))\n",
    "    except ZeroDivisionError:\n",
    "        precision = 1\n",
    "    recall = 1.0 * sum(true_pos) / (sum(true_pos) + sum(false_neg))\n",
    "    plt.title('Precision: %0.2f, Recall: %0.2f' % (precision, recall))\n",
    "    \n",
    "interact(plot_threshold, threshold=(0, 1, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# compute the relevant stats\n",
    "precisions, recalls, thresholds = metrics.precision_recall_curve(y, x)\n",
    "thresholds = np.hstack([[0.], thresholds])  # n precisions but n-1 thresholds\n",
    "f1s = 2 * (precisions * recalls) / (precisions + recalls)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(thresholds, precisions, label='Precision')\n",
    "plt.plot(thresholds, recalls, label='Recall')\n",
    "plt.plot(thresholds, f1s, label='F1 Score')\n",
    "plt.legend(loc='lower left')\n",
    "plt.xlabel(\"Threshold\"); plt.ylabel(\" \")\n",
    "plt.title(\"Precision, Recall, and F1 Score vs Thresholds\")\n",
    "    \n",
    "fig = plt.figure()\n",
    "plt.plot(recalls, precisions)\n",
    "#plt.fill_between(recalls, precisions, alpha=0.2)  # AUC value\n",
    "plt.xlim([0., 1.]); plt.ylim([0., 1.])\n",
    "plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision vs Recall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Given that we have a precision and recall tradeoff for probabilistic estimators, we usually report statistics like \"Precision at .6\", which means \"Precision when the threshold is set at Recall=.6\" and vice versa for \"Recall at .8\".\n",
    "\n",
    "**Questions (Part 2)**:\n",
    "1. Would you want high precision or high recall process for email spam detection?\n",
    "1. What about drug approvals?\n",
    "1. Let's suppose the NSA has an estimator for \"likely to be a terrorist\" which they use to determine who should be surveilled.  How do enhanced national security versus fourth-amendment protections map onto precision and recall?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Single-valued Probabilistic Prediction Metrics\n",
    "\n",
    "While a modeler can decide the appropriate threshold once given a precision-recall curve, it is hard to optimize for and it doesn't necessarily make sense to optimize for \"Precision at .6\" (why not \"Precision at .7\"?).  We need a single-valued metric that is independent of threshold.  Fortunately, there are two common ones:\n",
    "\n",
    "1. The **Area under the Curve** or **AUC** computes the area under the Precision Recall curve.\n",
    "1. There is a **Receiver Operating Charateristic**, which is similar to the Precision-Recall curve.  The area under this curve is itself a metric called **ROC-AUC**.  The definition isn't hard, but it's beyond the scope of this course.  You can find out more [on Wikipedia](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) or develop some geometric intuition from this [blog post](https://shapeofdata.wordpress.com/2015/01/05/precision-recall-aucs-and-rocs/).\n",
    "1. The **Log-Loss** or **Cross Entropy** is another characteristic.  It is related to the notion of Entropy in Thermodynamics and Shannon-Entropy.  \n",
    "\n",
    "#### Entropy\n",
    "\n",
    "We can think of both entropy and cross entropy as measures of the cost of identifying an event or class. If the predicted probability of a class label is the same as the true probability of a class label, then the cost associated with predicting that label is given by the entropy:\n",
    "\n",
    "$$ - \\sum_j^N \\left[p_j \\log(p_j) + (1-p_j) \\log(1-p_j)\\right] $$\n",
    "\n",
    "where $p$ is the probability and $j$ corresponds to an observation or instance. The value of each summand will lie somewhere on the curve below. Notice how the curve is symmetric about its maximum, $p=0.5$. The cost is highest when the probability of correctly identifying a class is a toss-up ($p=0.5$) and lowest when the probability of correctly identifying a class is high ($p \\approx 1.0$) or incorrectly identifying a class is low ($p \\approx 0.0$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Plot entropy\n",
    "p = np.linspace(0,1)\n",
    "y_log = np.log(p)\n",
    "y_entropy = (-1)*(p*np.log(p) + (1-p)*np.log(1-p))\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(p,y_log)\n",
    "plt.title('Log(p)')\n",
    "plt.xlabel(\"Probability\"); plt.ylabel(\" \")\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(p,y_entropy)\n",
    "plt.title('Entropy')\n",
    "plt.xlabel(\"Probability\"); plt.ylabel(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Cross Entropy\n",
    "\n",
    "Entropy and cross entropy are closely related. When the probability distributions of the true and predicted classes are not the same, we use cross entropy. In this case, the true probabilities are either $0$ or $1$ while the predicted probabilities can be values on the interval $[0,1]$. For a binary class problem where $y_j$ is either $0$ or $1$, the cross entropy or log loss is given by:\n",
    "\n",
    "$$ - \\sum_j \\left[y_j \\log(p_j) + (1-y_j) \\log(1-p_j)\\right] $$\n",
    "\n",
    "**Questions:** \n",
    "1. Can you generalize entropy formula from a two-class metric to an $m$-class metric?  What about for the other metrics?\n",
    "1. In Scikit Learn, `metrics.auc` (computed from precision and recall numbers) and `metrics.average_precision_score` (computed from observations and predictions) returns the same score.  Why is average precision the same as AUC?\n",
    "1. Which of these metrics do you want to increase, and which do you want to decrease?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# AUC, ROC-AUC, Log-loss\n",
    "\n",
    "# for entropy, we need the predictions and observations by-class\n",
    "# e.g. two columns for a binomial prediction.\n",
    "x_full = np.vstack([x, 1-x]).T\n",
    "y_full = np.vstack([y, 1-y]).T\n",
    "\n",
    "# You can compute AUC either via the AUC function by the average_precision_score\n",
    "print \"AUC by hand:\", metrics.auc(recalls, precisions)\n",
    "print \"AUC:\", metrics.average_precision_score(y, x)\n",
    "print \"ROC-AUC:\", metrics.roc_auc_score(y, x)\n",
    "print \"Entropy:\", metrics.log_loss(y_full, x_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Feature importance\n",
    "\n",
    "Another common metric we usually want to understand is how important a specific feature or signal is.\n",
    "1.  One common general technique is to leave a column out, retrain the model, and measure the drop in the metric.  This can be expensive because it requires retraining a model.\n",
    "1.  Another common technique is to use the same model but feed in data with one column randomly shuffled.  Measuring the loss on the performance metric compared with feeding in the non-shuffled data is another performance metric.\n",
    "\n",
    "**Question:**\n",
    "1. How would you do this for a linear model?\n",
    "1. Why is leave one out analysis computationally expensive?\n",
    "1. When using leave a column out, what happens if two columns are either identical or nearly identical aliases for one another?\n",
    "1. How does randomly shuffling data in a column affect the performance of the predictions?\n",
    "1. What are some other strategies that might counter some of the shortcomings of the above techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Nonparametric vs. parametric analysis\n",
    "\n",
    "One of the key distinctions in stats is between parametric and non-parametric statistics.  Rather than making anything specific distributional assumptions (e.g. in Hypothesis testing, we assume that we're seeing normally distributed data) or model form assumptions (e.g. in Linear Regression, we are assuming linear relationship between the dependent and independent variables), we do statistics without making those assumptions.  For example, we can use non-parametric statistics techniques like [Kernel Density Estimation](http://en.wikipedia.org/wiki/Kernel_density_estimation) to compare two distributions or use [Gaussian Process Regression](http://en.wikipedia.org/wiki/Gaussian_process_regression) (Scikit documentation [available here](http://scikit-learn.org/stable/modules/gaussian_process.html)) to perform regression analysis.  We won't talk about non-parametric statistics too much here but it's a good thing to know this distinction exists.\n",
    "\n",
    "Usually, by putting in parametric assumptions, we find it easier to learn about our data by introducing a prior (in the Bayesian sense) into our modeling assumptions.  However, if this prior is incorrect (e.g. the data isn't well-explained by a linear relationship), then we could be going astray.\n",
    "\n",
    "## Asymptotic approximation property\n",
    "\n",
    "Some models, e.g. Linear Regression, have floor for how much better they are able to fit the data, even with larger amounts of it.  If the underlying model is not linear, then no amount of data will make the model fit the data better.  In general, decision trees, boosting algorithms, and neural networks do satisfy this property.  What these models gain in precision, they often loose in explainability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Exit Tickets\n",
    "1. How does the choice of error metric influence the effect of outliers in the data?\n",
    "1. Describe to your grandma what precision and recall are, and the precision-recall tradeoff.\n",
    "1. What metrics can I use for probabilistic predictions if I don't want to use a threshold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Spoilers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Answers\n",
    "\n",
    "1. Regression: Pricing, population.  Classification: spam, product choice\n",
    "1. Clustering, dimensional reduction\n",
    "\n",
    "### Metrics for Regression\n",
    "\n",
    "1. Minimizing the absolute error finds the median.  Consider the \"derivative\" of $$\\left|z - z_j\\right| $$ with respect to $z$.  The median is less susceptible to outliers.  This gives some explanation of why (outliers have a linear, rather than quadratic effect).\n",
    "1. $R^2$ is invariant to $y$, Absolute Error linearly, and MSE quadratically.\n",
    "\n",
    "### Metrics for Class Predictions\n",
    "1. Precision gives what fraction of positive predictions we got right.  Recall tells us how many true positives we were able to find.\n",
    "1. High precision: When a false positive is dangerous.  High recall: When a false negative is dangerous.\n",
    "1. Perhaps controversial but I would say Harvard is a high precision but not high recall.  here are more good students than available slots.  Admitting a poor student (low precision) takes a slot away from a good student, but missing a good student (low recall) still leaves them with a good class.  And that student will do fine at Stanford. A large state school might be higher recall but lower precision.\n",
    "1. Blackstone argued for a high precision process in judicial proceedings, at the expense of recall.  Renault wants a high recall process, perhaps at the expense of precision. This is considering classification *as criminals*.  Considering classification *as innocents* would invert this.\n",
    "\n",
    "### Precision-Recall Tradeoff\n",
    "\n",
    "Part 1:\n",
    "\n",
    "1. Increasing $\\underline p$ increases precision (as you drop some false positives) and decreases recall (as you add false negatives).  Here, we are assuming that the estimator is halfway decent.\n",
    "1. A perfect estimator would be a horizontal line across the top of the graph.  If it were guessing at random, it would be a horizontal line at the global rate of positive examples (precision at recall = 1.). The precision isn't dependent on the threshold, since the positive predictions are random.  The recall will still vary, as a low or high threshold will accept or reject all predictions.\n",
    "1. Using $1-f$ will result in an estimator that is below the line of a random estimator (i.e. it is strictly worse than just guessing).\n",
    "1. We want a high precision process when the cost of a false positive is high.  We want a high recall process when the cost of a false negative is high.\n",
    "\n",
    "Part 2:\n",
    "\n",
    "1. You want a high precision Spam Detector (better to read a few extra spam emails then that email telling you about your promotion).\n",
    "1. FDA Drug Approvals are very risk averse: they are looking for a high precision process.\n",
    "1. Optimizing for enhanced national security, you want high recall (false negatives are terrorist attacks).  We want to find all terrorists. Optimizing for civil liberties, you want high precision (false positives are essentially violations of civil liberties). We don't want non-terrorists arrested.\n",
    "\n",
    "### Single-valued Probabilistic Prediction Metrics\n",
    "1.  If $p_{jk}$ is the probability that example $j$ will be in class $k$, then the entropy is\n",
    "    $$ \\sum_{jk} y_{jk} \\log(p_{jk}) \\,.$$\n",
    "    where $y_{jk}$ is a flag saying if observation $j$ is in class $k$. Remember that $\\sum_k p_{jk} = 1$.\n",
    "1.  The average precision means precision integrated over recall.  If you draw out the picture, it is exactly the area under the precision recall curve.\n",
    "1.  In `sklearn`, metrics you wish to increase usually end in `*_score` while metrics you wish to decrease usually end in `*_error`. Increase: AUC, ROC. Decrease: Log loss.\n",
    "\n",
    "### Feature Importance\n",
    "1. Multiply the feature weight and the standard deviation of the data for that feature. This indicates how much the result will change due to that feature.\n",
    "1. You need to retrain the model for each column.  The random shuffle just needs a call of predict.\n",
    "1. The columns provide little marginal signal given the other so they would both be deemed ineffective - however, removing both columns could significantly harm the performance of the model).  \n",
    "1. You will notice importance a bit more directly, but there may still be problems of aliasing.\n",
    "1. Dimensional reduction, like PCA, can give the relevant independent dimensions to use for the model.\n",
    "\n",
    "### Exit Tickets Answers\n",
    "1. MSE emphasizes them, absolute error reduces their influence.\n",
    "1. See question 1 in metrics for class predictions and question 1 (part 1) for precision-recall tradeoff. \n",
    "1. Entropy, AUC, ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*Copyright &copy; 2015 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
