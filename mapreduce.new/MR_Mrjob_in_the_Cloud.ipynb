{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# mrjob in the Cloud\n",
    "\n",
    "[mrjob](https://github.com/Yelp/mrjob) is a Python library originally developed by Yelp to streamline the writing and execution of Hadoop Streaming jobs both locally and on the cloud. Instead of directly callng the Hadoop Streaming API, mrjob allows us to write Python code and handle much of the cluster configuration automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Amazon's Cloud Computing Services\n",
    "\n",
    "Amazon Web Services has extremely thorough documentation around everything from the commands available to the command line interface (CLI) `aws {commands}`, to the Python wrapper for said interface `boto`, to full tutorials and examples on how to fire up an EMR cluster or a bunch of EC2 instances with almost any desired data processing framework.\n",
    "\n",
    "EC2 is cheaper than EMR, but EMR is recommended for immediate use of Hadoop and any other project in the ecosystem (configurable for your cluster via [Amazon Machine Images](http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/ami-versions-supported.html) (AMIs). In a production setting it's possible you'll want to use specific versions for consistency; in our case it's safe to use the most recent version (`3.6.0` at the time of this writing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Setting up a personal AWS account\n",
    "\n",
    "To use AWS you'll need to [create an account](http://aws.amazon.com/) if you haven't already. For the first year after new account creation, you'll be eligible for discounts on some services as part of the Free Tier program.\n",
    "\n",
    "Access the AWS [web console](https://console.aws.amazon.com/s3/) to handle most of your configuration. You'll need at least one S3 bucket to serve as storage for your logs and output.\n",
    "\n",
    "From there you can create EMR clusters as you wish and run jobs. Be careful about the nodes you use, as only certain sizes are eligible for the free tier discounts. Still, you only pay for what you use, and the costs for small, educational jobs are relatively manageable.\n",
    "\n",
    "There's an in depth [tutorial](http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-get-started.html) available, and more detailed cluster configuration information can be found in this notebook, and in the Spark module.\n",
    "\n",
    "Also note that in addition to the normal credentials you might need to take care of:\n",
    "* Generating an EC2 keypair (this is separate from the AWS general keypair and goes in .mrjob.conf)\n",
    "* `aws emr create-default-roles` if you plan on just using the defaults for EMR\n",
    "* Making sure your user is part of a group with sufficient permissions (admin is probably fine)\n",
    "\n",
    "### Managing AWS credentials on Digital Ocean\n",
    "\n",
    "\"If you want to use your personal AWS keys, you can overwrite `~/.mrjob.conf` with yours. Keys for our S3 bucket are saved under `~/.aws/config`. If you ever want to run mrjob using our keys for S3 access, you can simply remove the keys from `~/.mrjob.conf` and (according to the mrjob docs), the keys in `~/.aws/config` will take precedence.\"\n",
    "\n",
    "In general, make sure that the configuration in `~/.mrjob.conf` makes sense.\n",
    "\n",
    "`- sudo apt-get install -y python-pip || sudo yum install -y python-pip` is a more robust bootstrapping statement.\n",
    "\n",
    "You can set up multiple profiles in the `~/.aws/credentials` file in order to facilitate copying data from our S3 bucket while still being able to access your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### AWS credentials and command line tools\n",
    "\n",
    "1. To verify that it is working, try \n",
    "``` bash\n",
    "aws s3 ls\n",
    "```\n",
    "You should get back a json blob that contains the permissions you just added for your user.  If not, double-check that you got your permissions setup correctly.\n",
    "\n",
    "1. `boto` ([docs](https://boto.readthedocs.org/en/latest/)) is a python library that wraps the functionality of `awscli`.  You can install it using\n",
    "``` bash\n",
    "pip install boto\n",
    "```\n",
    "and follow the instructions in the docs to get started.\n",
    "\n",
    "1. Another option for interacting with s3 from the command line is `s3cmd`. You can download/start using it via   \n",
    "``` bash\n",
    "git clone https://github.com/s3tools/s3cmd.git\n",
    "```\n",
    "and follow the documentation [here](https://github.com/s3tools/s3cmd).\n",
    "\n",
    "### Python `mrjob`\n",
    "\n",
    "To test it, clone this [github repo](https://github.com/Yelp/mrjob) and run wordcount on README.rst:\n",
    "```bash\n",
    "git clone https://github.com/Yelp/mrjob.git\n",
    "cd mrjob\n",
    "\n",
    "# run command locally\n",
    "# this is good for testing and debugging on files locally\n",
    "python examples/mr_word_freq_count.py README.rst\\\n",
    "   --no-output --output-dir=/tmp/wc/\n",
    "   \n",
    "# check the output file contents:\n",
    "cat /tmp/wc/* | more\n",
    "\n",
    "# run command on ec2 and write output to our s3 bucket\n",
    "# this costs money so only do it when you have working code\n",
    "\n",
    "python examples/mr_word_freq_count.py -r emr README.rst \\\n",
    "   --no-output --output-dir=s3://dataincubator-fellow/<user>-wc/\n",
    "```\n",
    "Note: if you're unable to start a new jobflow, use the `check_emr_jobflows.py` script in `datacourse/scripts/` and explicitly join the shortest queue by adding the flag `--emr-job-flow-id=j-JOBFLOWID`.\n",
    "\n",
    "### check the output file contents:\n",
    "```bash\n",
    "aws s3 ls s3://dataincubator-fellow/<user>-wc/\n",
    "aws s3 cp --recursive s3://dataincubator-fellow/<user>-wc/ /tmp/<user>-wc/\n",
    "```\n",
    "Note: be sure to fill in `<user>` with a key that is unique to you.\n",
    "\n",
    "A few notes:\n",
    "1. You can also upload files to s3 using the AWS CLI so that your entire workflow can be on s3.\n",
    "1. The server will take a while to boot up the first time but it will stay alive.  Any subsequent jobs that are submitted will not have to reboot.  If there are already jobs running, it will wait up to 5 minutes before spawning another server (please be patient).  It will stay idle for 2 hours and then kill itself.\n",
    "1. Take a look at `examples/mr_word_freq_count.py`.  This is the simple \"word count\" mapreduce.\n",
    "\n",
    "## Useful AWS CLI commands\n",
    "\n",
    "- Access Hadoop web UI, e.g. ResourceManager\n",
    "    - Option 1: via a local port\n",
    "```bash \n",
    "ssh -L 8158:ec2-52-0-25-37.compute-1.amazonaws.com:9026 hadoop@ec2-52-0-25-37.compute-1.amazonaws.com -i /path/to/fellos201501.pem\n",
    "```\n",
    "Now in your browser, go to the address `localhost:8158`  \n",
    "\n",
    "    - Option 2: via dynamic port forwarding\n",
    "        1. Type: \n",
    "```bash\n",
    "aws emr socks --cluster-id j-XXXX --key-pair-file ~/path/to/keypair.pem\n",
    "```\n",
    "        1. Then follow [these steps)(http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-connect-master-node-proxy.html) to add FoxyProxy to your Chrome/Firefox browser to natively use the web UI.\n",
    "\n",
    "\n",
    "- `ssh` into master node to access HDFS, Pig interactive, etc.\n",
    "\n",
    "```bash\n",
    "ssh hadoop@{master-public-dns}.compute-1.amazonaws.com -i /path/to/pemfile.pem\n",
    "```\n",
    "\n",
    "- One-liner to preview a .gz file:\n",
    "```bash\n",
    "s3cmd get s3://dataincubator-course/wikidata/wikistats/pagecounts/pagecounts-20081208-030000.gz - | gunzip -c | less\n",
    "```\n",
    "\n",
    "### Third party software\n",
    "\n",
    "There are some third-party tools that can help navigate AWS S3. It can be time-consuming to go through the command line looking for logs when there's no autocomplete or easily viewable directory structure - in which case something like Bucket Explorer might save you some time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Google Cloud Platform\n",
    "\n",
    "Cloud Dataproc is GCP's analog to EMR: a managed Hadoop cluster environment that uses Google Compute Engine instances under the hood. There's a comprehensive 60-day free trial with $300 of credit to use (which should be more than plenty for our purposes).  You can interact with the comput engine either through the [Cloud SDK website](https://cloud.google.com/sdk/#Quick_Start) or through the `gcloud` command line tool.  Here are some step-by-step instructions to get started, largely using the website.\n",
    "\n",
    "1. Go to the [Cloud SDK website](https://cloud.google.com/sdk/#Quick_Start) and sign up for a trial account. It will request your credit card information, but you will not be charged.\n",
    "\n",
    "1. In the top bar of the website, there should be a \"Go to projects\" drop down.  Open it and select \"Create a project...\".  You will be prompted for a name, and then allowed to create the project.  The project acts as an umbrella for various related resources.\n",
    "\n",
    "1. From the menu in the upper left, select \"Storage\", and then create a bucket.  This is essentially a cloud filesystem onto which you can load files and directories.  Give it a unique name.\n",
    "\n",
    "1. From the same menu in the upper left, select \"API Manager\".  Ensure that the \"Compute Engine API\", the \"Cloud Dataproc API\", and the  \"Cloud Storage JSON API\" are all enabled.\n",
    "\n",
    "1. Now, switch to the command line on your DO box.  Run `gcloud init`.  It will ask you to authenticate with your Google account, as well as set the default project and compute zone.\n",
    "\n",
    "  1. This should set things up to authenticate all gcloud commands.  But if you're having trouble, go to the \"Credentials\" tab in the \"API Manager\" section of the website.  Chose \"Create Credentials\" and choose \"Service account key\". Then, select New service account, enter a Name and select Key type JSON. Put this JSON file somewhere and point to it with the environment variable $GOOGLE_APPLICATION_CREDENTIALS (eg. `export GOOGLE_APPLICATION_CREDENTIALS=/home/vagrant/auth/gcp.json`). Make sure the service account has editor-level permissions.\n",
    "\n",
    "1. Upload data to the bucket you created above, using the `gsutil cp` command.  It can take as arguments local paths, Google Storage paths (`gs://bucket-name/path`), and Amazon S3 paths (`s3://bucket-name/path`).  Useful flags include `-r` (recursive) and `-m` (parallelize tasks).  For example, `gsutil -m cp -r s3://dataincubator-course/mrdata/english/ gs://mybucket/data/wikipedia/`.\n",
    "\n",
    "1. Run a job with MRJob.  You can either let MRJob create a cluster for you, or you can provision one by hand.  The latter is recommended, as it is a bit easier to configure.\n",
    "\n",
    "  1. If no cluster is specified, MRJob will create a cluster for you.  For example,\n",
    "  ```bash\n",
    "      python script.py -r dataproc \\\n",
    "          gs://bucket/directory > out.txt\n",
    "  ```\n",
    "  To use third-party libraries you'll need to configure the bootstrapping option in `~/.mrjob.conf`. You can also specify cluster creation options here. For the free trial, remember that not all options are available.\n",
    "  \n",
    "  1. You can create a cluster using the Cloud SDK Website.  From the main menu, select \"Dataproc\", and then choose \"Create cluster\".  You can configure the cluster, including using SSDs, which often helps performance.  Then when launching the job, you specify the cluster ID:\n",
    "  ```bash\n",
    "      python script.py -r dataproc \\\n",
    "          --cluster-id cluster-1 \\\n",
    "          gs://bucket/directory > out.txt\n",
    "  ```\n",
    "  Before you do so, you need to be aware that the cluster will not be provisioned with many libraries.  Critically, mrjob itself isn't included.  The easiest solution is to create a provisioning script that looks something like this:\n",
    "  ```bash\n",
    "      #!/bin/bash\n",
    "      curl https://bootstrap.pypa.io/get-pip.py | python\n",
    "      pip install mrjob\n",
    "      pip install simplejson\n",
    "      pip install mwparserfromhell\n",
    "      pip install lxml\n",
    "  ```\n",
    "  Upload this script to a Google Storage bucket (`gsutil cp`).  When you are creating a cluster, select the advanced options (\"Preemptible workers, bucket, network, version, initialization, & access options\") and enter the `gs://` path to the init script in the \"Initialization actions\" entry.  This will cause each node in the cluster to run this script, installing pip, mrjob, etc.\n",
    "  \n",
    "    **Remember:** The cluster will not be shut down after your job completes.  Destroy it, from the web console or the command line, to avoid unnecessary charges.\n",
    "\n",
    "There is a [quickstart guide](https://pythonhosted.org/mrjob/guides/dataproc-quickstart.html) to using mrjob with Dataproc.  The full [mrjob documentation](https://media.readthedocs.org/pdf/mrjob/latest/mrjob.pdf) contains plenty more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### <a name=\"upgrading\"></a>Upgrading from the free trial\n",
    "\n",
    "The free trial limits you to 8 YARN cores, including the master node, which realistically means the biggest cluster you can use on dataproc has 3 worker instances with 2 nodes each.\n",
    "\n",
    "It is a significant time saver to upgrade to a paid account. You will keep your free trial credit (it expires when your trial would have expired), the only difference is **you will have to manually cancel your account** to avoid being billed after the trial period expires. Doing this is painless and will increase your quota to 24 YARN cores.\n",
    "\n",
    "### Viewing logs\n",
    "\n",
    "mrjob tries to pull back the actual Python errors Tracebacks when a job fails, but as of this writing, this is not implemented for Dataproc.  You can find the logs by SSHing into the worker nodes and examining them directly, but it may be slightly easier to use the log viewer in the web console.  Go to the [Logging page](https://console.cloud.google.com/logs) of the console and select \"Dataproc\" from the drop-down menu.  You can then select a specific cluster to search, or you can search all clusters.  Searching for \"Traceback\" will yield all lines that open a Python traceback, but you can't see the whole error message from this view.  Instead, open that line and find the filename, in the structPayload structure.  Then search for that filename.  You'll get all the lines of that file, in order, so you can figure out what the error actually was.\n",
    "\n",
    "Another option: You can open the YARN web interface by following the steps documented by GCP here:\n",
    "https://cloud.google.com/dataproc/docs/concepts/cluster-web-interfaces\n",
    "Some logging aggregation is disabled by default on GCP, but you can determine where tasks failed, SSH into worker nodes a little more carefully, and look at logs from there. It's also useful for keeping track of your jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*Copyright &copy; 2015 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
