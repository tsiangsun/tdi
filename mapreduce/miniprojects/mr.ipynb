{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import grader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Mapreduce\n",
    "\n",
    "## Introduciton\n",
    "\n",
    "We are going to be running mapreduce jobs on the wikipedia dataset.  The dataset is available (pre-chunked) on S3: `s3://dataincubator-course/mrdata/simple/`.  It may be downloaded with the `aws s3 sync` command or via HTTPS from `https://s3.amazonaws.com/dataincubator-course/mrdata/simple/part-000*`.\n",
    "\n",
    "For development, you can even use a single chunk (eg. part-00026.xml.bz2). That is small enough that mrjob can process the chunk in a few seconds. Your development cycle should be:\n",
    "\n",
    "1.  Get your job to work locally on one chunk.  This will greatly speed up your\n",
    "development.  To run on local:\n",
    "```bash\n",
    "python job_file.py -r local data/wikipedia/simple/part-00026.xml.bz2 > /tmp/output.txt\n",
    "```\n",
    "    \n",
    "2.  Get your job to work on the full dataset on GCP (Google Cloud Platform).  This will greatly speed up your production.  To run on GCP ([details](https://pythonhosted.org/mrjob/guides/dataproc-quickstart.html)):\n",
    "```bash\n",
    "python job_file.py -r dataproc data/wikipedia/simple/part-00026.xml.bz2 \\\n",
    "    --output-dir=gs://my-bucket/output/ \\\n",
    "    --no-output \n",
    "```\n",
    "\n",
    "    Not that you can also pass an entire local directory of data (eg. `data/simple/`) as the input.\n",
    "\n",
    "### Note on Memory\n",
    "There's a large difference between developing locally on one chunk and running your job on the entire dataset.  While you can get away with sloppy memory use locally, you really need to keep memory usage down if you hope to be able to complete the miniproject.  Remember, memory needs to be $O(1)$, not $O(n)$ in input.\n",
    "\n",
    "### Multiple Mapreduces\n",
    "You can combine multiple steps by overriding the [steps method](https://pythonhosted.org/mrjob/guides/writing-mrjobs.html#multi-step-jobs).  Usually your mapreduce might look like this\n",
    "```python\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class SingleMRJob(MRJob):\n",
    "    def mapper(self, key, value):\n",
    "        pass\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        pass\n",
    "```\n",
    "\n",
    "`MRJob` automatically uses the `mapper` and `reducer` methods.  To specify multiple steps, you need to override the `steps` method:\n",
    "\n",
    "```python\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class MultipleMRJob(MRJob):\n",
    "    def mapper1(self, key, value):\n",
    "        pass\n",
    "\n",
    "    def reducer1(self, key, values):\n",
    "        pass\n",
    "        \n",
    "    def mapper2(self, key, value):\n",
    "        pass\n",
    "\n",
    "    def reducer2(self, key, values):\n",
    "        pass\n",
    "        \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper1, reducer=self.reducer1),\n",
    "            MRStep(mapper=self.mapper2, reducer=self.reducer2),\n",
    "        ]\n",
    "```\n",
    "\n",
    "As a matter of good style, we recommend that you actually write each individual mapreduce as it's own class.  Then write a wrapper module whose sole job is to combine those mapreduces by overriding `steps`.\n",
    "\n",
    "Some simple boilerplate for this, taking advantage of the default `steps` function that we get for free in a single-step MRJob class:\n",
    "\n",
    "```python\n",
    "class FirstStep(MRJob):\n",
    "  def mapper(self, key, value):\n",
    "    pass\n",
    "  def reducer(self, key, values):\n",
    "    pass\n",
    "  \n",
    "class SecondStep(MRJob):\n",
    "  def mapper(self, key, value):\n",
    "    pass\n",
    "  def reducer(self, key, values):\n",
    "    pass\n",
    "  \n",
    "class SteppedJob(MRJob):\n",
    "  \"\"\"\n",
    "  A two-step job that first runs FirstStep's MR and then SecondStep's MR\n",
    "  \"\"\"\n",
    "  def steps(self):\n",
    "    return FirstStep().steps() + SecondStep().steps()\n",
    "```\n",
    "\n",
    "\n",
    "### Note on Style\n",
    "Here are some helpful articles on how mrjob works and how to pass parameters to your script:\n",
    "  - [How mrjob is run](https://pythonhosted.org/mrjob/guides/concepts.html#how-your-program-is-run)\n",
    "  - [Adding passthrough options](https://pythonhosted.org/mrjob/job.html#mrjob.job.MRJob.add_passthrough_option)\n",
    "  - [An example of someone solving similar problems](http://arunxjacob.blogspot.com/2013/11/hadoop-streaming-with-mrjob.html)\n",
    "\n",
    "See the notebook \"Hadoop MapReduce with mrjob\" in the datacourse for more details.\n",
    "\n",
    "Finally, if you are find yourself processing a lot of special cases, you are probably doing it wrong.  For example, mapreduce jobs for `Top100WordsSimpleWikipediaPlain`, `Top100WordsSimpleWikipediaText`, and `Top100WordsSimpleWikipediaNoMetaData` are less than 150 lines of code (including generous blank lines and biolerplate code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://dataincubator-course/mrdata/simple/part-00009.xml.bz2 to ./part-00009.xml.bz2\n",
      "download: s3://dataincubator-course/mrdata/simple/part-00008.xml.bz2 to ./part-00008.xml.bz2\n",
      "download: s3://dataincubator-course/mrdata/simple/part-00006.xml.bz2 to ./part-00006.xml.bz2\n",
      "download: s3://dataincubator-course/mrdata/simple/part-00002.xml.bz2 to ./part-00002.xml.bz2\n",
      "download: s3://dataincubator-course/mrdata/simple/part-00003.xml.bz2 to ./part-00003.xml.bz2\n",
      "download: s3://dataincubator-course/mrdata/simple/part-00004.xml.bz2 to ./part-00004.xml.bz2\n",
      "download: s3://dataincubator-course/mrdata/simple/part-00000.xml.bz2 to ./part-00000.xml.bz2\n",
      "download: s3://dataincubator-course/mrdata/simple/part-00001.xml.bz2 to ./part-00001.xml.bz2\n",
      "download: s3://dataincubator-course/mrdata/simple/part-00007.xml.bz2 to ./part-00007.xml.bz2\n",
      "download: s3://dataincubator-course/mrdata/simple/part-00005.xml.bz2 to ./part-00005.xml.bz2\n",
      "download: s3://dataincubator-course/mrdata/simple/part-00015.xml.bz2 to ./part-00015.xml.bz2\n",
      "download: s3://dataincubator-course/mrdata/simple/part-00014.xml.bz2 to ./part-00014.xml.bz2\n",
      "download: s3://dataincubator-course/mrdata/simple/part-00010.xml.bz2 to ./part-00010.xml.bz2\n",
      "download: s3://dataincubator-course/mrdata/simple/part-00011.xml.bz2 to ./part-00011.xml.bz2\n",
      "download: s3://dataincubator-course/mrdata/simple/part-00016.xml.bz2 to ./part-00016.xml.bz2\n",
      "download: s3://dataincubator-course/mrdata/simple/part-00012.xml.bz2 to ./part-00012.xml.bz2\n",
      "download: s3://dataincubator-course/mrdata/simple/part-00013.xml.bz2 to ./part-00013.xml.bz2\n",
      "download: s3://dataincubator-course/mrdata/simple/part-00017.xml.bz2 to ./part-00017.xml.bz2\n",
      "download: s3://dataincubator-course/mrdata/simple/part-00018.xml.bz2 to ./part-00018.xml.bz2\n",
      "download: s3://dataincubator-course/mrdata/simple/part-00019.xml.bz2 to ./part-00019.xml.bz2\n",
      "download: s3://dataincubator-course/mrdata/simple/part-00023.xml.bz2 to ./part-00023.xml.bz2\n",
      "download: s3://dataincubator-course/mrdata/simple/part-00021.xml.bz2 to ./part-00021.xml.bz2\n",
      "download: s3://dataincubator-course/mrdata/simple/part-00024.xml.bz2 to ./part-00024.xml.bz2\n",
      "download: s3://dataincubator-course/mrdata/simple/part-00025.xml.bz2 to ./part-00025.xml.bz2\n",
      "download: s3://dataincubator-course/mrdata/simple/part-00022.xml.bz2 to ./part-00022.xml.bz2\n",
      "download: s3://dataincubator-course/mrdata/simple/part-00026.xml.bz2 to ./part-00026.xml.bz2\n",
      "download: s3://dataincubator-course/mrdata/simple/part-00020.xml.bz2 to ./part-00020.xml.bz2\n"
     ]
    }
   ],
   "source": [
    "!aws s3 sync s3://dataincubator-course/mrdata/simple/ . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "WORD_RE = re.compile(r\"[\\w]+\")\n",
    "\n",
    "for word in WORD_RE.findall('input.txt'):\n",
    "    print word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#test.py\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRWordFrequencyCount(MRJob):\n",
    "    def mapper(self, _, line):\n",
    "        yield \"chars\", len(line)\n",
    "        yield \"words\", len(line.split())\n",
    "        yield \"lines\", 1\n",
    "        \n",
    "    def reducer(self, key, values):\n",
    "        yield key, sum(values)\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "#    MRWordFrequencyCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /home/vagrant/.mrjob.conf\n",
      "Creating temp directory /tmp/top100_words_simple_plain.vagrant.20170703.215042.794552\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "Streaming final output from /tmp/top100_words_simple_plain.vagrant.20170703.215042.794552/output...\n",
      "Removing temp directory /tmp/top100_words_simple_plain.vagrant.20170703.215042.794552...\n"
     ]
    }
   ],
   "source": [
    "!python top100_words_simple_plain.py /home/vagrant/datacourse/mapreduce/miniprojects/part-* > top100.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 1596419), ('quot', 1400092), ('gt', 1211888), ('lt', 1205656), ('id', 1142905), ('of', 972204), ('in', 659218), ('and', 634202), ('text', 604352), ('a', 581510), ('title', 539917), ('to', 489127), ('page', 439939), ('is', 407340), ('format', 386311), ('model', 381564), ('category', 380599), ('revision', 380467), ('ns', 378544), ('timestamp', 377863)]\n"
     ]
    }
   ],
   "source": [
    "def question1():\n",
    "    with open('top100.txt') as f:\n",
    "        content = f.readlines()\n",
    "    res1=[]\n",
    "\n",
    "    content = [x.strip() for x in content] \n",
    "    for line in content:\n",
    "        l = line.split()\n",
    "        res1.append((l[0].strip('\"'),int(l[1])))\n",
    "    return res1\n",
    "\n",
    "result1 = question1()\n",
    "print result1[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Question 1: top100_words_simple_plain\n",
    "Return a list of the top 100 words in an article text (in no particular order). You will need to write this as two map reduces:\n",
    "\n",
    "1. The first job is similar to standard wordcount but with a few tweaks. The data provided for wikipedia is in `*.xml.bz2` format.  Mrjob will automatically decompress `bz2`.  We'll deal with the `xml` in the next question. For now, just treat it as text.  A few hints:\n",
    "   - To split the words, use the regular expression \"\\w+\".\n",
    "   - Words are not case sensitive: i.e. \"The\" and \"the\" reference to the same word.  You can use `string.lower()` to get a single case-insenstive canonical version of the data.\n",
    "\n",
    "2. The second job will take a collection of pairs `(word, count)` and filter for only the highest 100.  A few notes:\n",
    "    - **Passing parameters:** To make the job more reusable make the job find the largest `n` words where `n` is a parameter obtained via [`get_jobconf_value`](https://pythonhosted.org/mrjob/utils-compat.html).\n",
    "    - **Keeping track of the top n:** We have to keep track of at most the `n` most popular words.  As long as `n` is small, e.g. 100, we can keep track of the *running largest n* in memory wtih a priority-queue. We suggest taking a look at `heapq` ([details](https://docs.python.org/2/library/heapq.html)), part of the Python standard library for this.  It allows you to push elemnets into a list while keeping track of the highest priority element.\n",
    "```python\n",
    "h = []\n",
    "heappush(h, (5, 'write code'))\n",
    "heappush(h, (7, 'release product'))\n",
    "heappush(h, (1, 'write spec'))\n",
    "heappush(h, (3, 'create tests'))\n",
    "heappop(h)  // returns (1, 'write spec')\n",
    "```\n",
    "   \n",
    "       A naive implementation would cost $O(1)$ to insert but $O(n)$ to retrieve.  `heapq` uses a [self-balancing binary search tree](https://en.wikipedia.org/wiki/Self-balancing_binary_search_tree) to enable $O(\\log(n))$ insertion and $O(1)$ removal. You may be asked about this data structure on an interview so it is good to get practice with it now.\n",
    "    - **Working across nodes:** To obtain the largest `n`, we need to first obtain the largest n elements per chunk from the mapper, output them to the same key (reducer), and then collect the largest n elements of those in the reducer (**Question:** why does this gaurantee that we have found the largest n over the entire set?)\n",
    "    - **Working within a node:** Given that we are using a priority queue, we will need to first initialize it, then `push` or `pushpop` each record to it, and finally output the top `n` after seeing each record.  For mappers, notice that these three phases correspond nicely to these three functions:\n",
    "        - `mapper_init`\n",
    "        - `mapper`\n",
    "        - `mapper_final`\n",
    "\n",
    "    There are similar functions in the reducer.  Also, while the run method to launch the mapreduce job is a classmethod:\n",
    "        ```python\n",
    "          if __name__ == '__main__':\n",
    "            MRWordCount.run()\n",
    "        ```\n",
    "     actual instances of our mapreduce are instantiated on the map and reduce nodes.  More precisely, a separate mapper class is instantiated in each map node and a reducer class is instantiated in each reducer node.  This means that the three mapper functions can pass state through `self`, e.g. `self.heap`. Remember that to pass state between the map and reduce phase, you will have to use `yield` in the mapper and read each line in the reducer. (**Question:** Can you pass state between two mappers?)\n",
    "\n",
    "**Checkpoint:**\n",
    "- Total unique words: 1,584,646"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score:  1.0\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "def top100_words_simple_plain():\n",
    "    return [(\"the\", 1586419)] * 100\n",
    "\n",
    "grader.score(question_name='mr__top100_words_simple_plain', func=question1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Question 2: top100_words_simple_text\n",
    "Notice that the words \"page\" and \"text\" make it into the top 100 words in the previous problem.  These are not common English words!  If you look at the xml formatting, you'll realize that these are xml tags.  You should parse the files so that tags like `<page></page>` should not be included in your total, nor should words outside of the tag `<text></text>`.\n",
    "\n",
    "**Hints**:\n",
    "1. Both `xml.etree.elementtree` from the Python stdlib or `lxml.etree` parse xml. `lxml` is significantly faster though and avoids some bugs.\n",
    "\n",
    "2. In order to parse the text, we will have to accumulate a `<page></page>` worth of data and then split the resulting string into words.\n",
    "\n",
    "3. Don't forget that the Wikipedia format can have multiple revisions but you only want the latest one.\n",
    "\n",
    "4. What happens if a content from a page is split across two different mappers? How does this problem scale with data size?\n",
    "\n",
    "**Checkpoint:**\n",
    "- Total unique words: 867,871"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of file 4668\n"
     ]
    }
   ],
   "source": [
    "from lxml import etree\n",
    "\n",
    "filename = 'part-00001.xml'\n",
    "\n",
    "def get_single_page(f):\n",
    "    page_single = []\n",
    "    flag = 0\n",
    "    line = ' '\n",
    "    while flag == 0 and line != '':\n",
    "        line = f.readline()\n",
    "        #print line\n",
    "        page_single.append(line)\n",
    "        if '</page>' in line:\n",
    "            flag = 1\n",
    "    return ''.join(page_single)\n",
    "    \n",
    "    \n",
    "    \n",
    "with open(filename, 'r') as f:\n",
    "    get_single_page(f)\n",
    "    count = 0\n",
    "    while True: #count < 3 :\n",
    "        page = get_single_page(f)\n",
    "        if page == '':\n",
    "            print 'end of file', count\n",
    "            break  \n",
    "        #print page\n",
    "        count += 1\n",
    "        root = etree.XML(page)\n",
    "        #print(root.tag)\n",
    "        content = root.xpath(\"//revision\")[-1].xpath(\".//text\")[0].text\n",
    "        \n",
    "        #outfile = open(filename+'_'+str(count), 'w')\n",
    "        #outfile.write(content.encode('ascii', 'ignore'))\n",
    "        #outfile.close()\n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open(filename, 'r') as f:\n",
    "    for line in f:\n",
    "        #print line,\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{{Infobox islands\n",
      "| name                = Wake Island\n",
      "| image name          = Wake Island map.png\n",
      "| image caption       = Map of Wake Island\n",
      "| image size          = 250px\n",
      "| location            = North\n"
     ]
    }
   ],
   "source": [
    "content = root.findall('revision')[-1].find('text').text\n",
    "print content[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{{Infobox islands\n",
      "| name                = Wake Island\n",
      "| image name          = Wake Island map.png\n",
      "| image caption       = Map of Wake Island\n",
      "| image size          = 250px\n",
      "| location            = North\n"
     ]
    }
   ],
   "source": [
    "#print root.xpath(\"//text\")[0].text\n",
    "content = root.xpath(\"//revision\")[-1].xpath(\".//text\")[0].text\n",
    "print content[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{{Infobox islands\n",
      "| name                = Wake Island\n",
      "| image name          = Wake Island map.png\n",
      "| image caption       = Map of Wake Island\n",
      "| image size          = 250px\n",
      "| location            = North Pacific\n",
      "| coordinates         = {{Coord|19|18|N|166|38|E|type:isle_region:UM-79|display=inline,title}}\n",
      "| total islands       = 3\n",
      "| area sqmi           = 2.85\n",
      "| coastline mi        = 12.0\n",
      "| coastline footnotes = <ref>Coastline for Wake Islet: {{convert|12.0|mi|abbr=on}}; Coastline for Wake Atoll: {{convert|21.0|mi|abbr=on}}</ref>\n",
      "| highest mount       = Ducks Point\n",
      "| elevation ft        = 20\n",
      "| country             = {{USA}} <br /> ''Wake Island is under the administration of the''  [[United States Air Force]]\n",
      "| population          = 150{{Citation needed|date=September 2011}}\n",
      "}}\n",
      "\n",
      "[[File:Wake Island air.JPG|thumb|upright=1.6|Aerial overview of the atoll]]\n",
      "'''Wake Island''' is an [[atoll]] (a type of [[island]]) in the [[Pacific Ocean]], near [[Hawaii]]. It is controlled by the [[United States Army]] and [[United States Air Force]]. It is a [[territory]] of the [[United States]], part of the [[United States Minor Outlying Islands]].\n",
      "\n",
      "== Geography ==\n",
      "Wake is located to the west of the [[International date line|International Date Line]] and sits in the [[Wake Island Time Zone]], one day ahead of the [[U.S. state|50 U.S. states]].\n",
      "\n",
      "Referring to the atoll as an island is the result of a pre-[[World War II]] desire by the [[United States Navy]] to distinguish Wake from other atolls, most of which were [[Japanese]] territory.\n",
      "\n",
      "== Reference ==\n",
      "{{reflist}}\n",
      "\n",
      "== Other websites ==\n",
      "* [http://weather.noaa.gov/weather/current/PWAK.html Current Weather, Wake Island]\n",
      "* [http://www.airnav.com/airport/PWAK AirNav – Wake Island Airfield] – Airport details, facilities and navigational aids\n",
      "* [http://www.astronautix.com/sites/waksland.htm Rocket launches at Wake Island]\n",
      "* [http://www.ibiblio.org/hyperwar/USMC/USMC-M-Wake.html The Defense of Wake] – United States Marine Corps historical monograph\n",
      "* [http://www.ibiblio.org/hyperwar/USMC/Wake/USMC-M-Wake-VI.html Surrender of Wake by the Japanese] – Marines in World War II\n",
      "* [http://www.smdc.army.mil/KWAJ/logistics/wake.html U.S. Army Strategic and Missile Defense Command] – Logistics, flight schedules, facilities\n",
      "* [http://www.wakeisland1975.com Photographic history of the 1975 Vietnamese refugee camp on Wake Island]\n",
      "* {{wikivoyage-inline|Wake Island}}\n",
      "* [http://www.pacificwrecks.com/provinces/wake.html Wake Island] – Pacific Wreck Database\n",
      "* {{IMDb title|0035530|Wake Island|(1942)}}\n",
      "* {{IMDb title|0372021|Wake Island: Alamo of the Pacific|(2003)}}\n",
      "\n",
      "{{US-geo-stub}}\n",
      "{{United States}}\n",
      "\n",
      "[[Category:Island insular areas of the United States]]\n",
      "[[Category:Micronesian islands]]\n",
      "[[Category:Atolls]]\n"
     ]
    }
   ],
   "source": [
    "lines = content.split('\\n')\n",
    "for l in lines:\n",
    "    print l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /home/vagrant/.mrjob.conf\n",
      "Creating temp directory /tmp/top100_words_simple_text.vagrant.20170705.223645.674896\n",
      "Running step 1 of 3...\n",
      "Running step 2 of 3...\n",
      "Running step 3 of 3...\n",
      "Streaming final output from /tmp/top100_words_simple_text.vagrant.20170705.223645.674896/output...\n",
      "Removing temp directory /tmp/top100_words_simple_text.vagrant.20170705.223645.674896...\n"
     ]
    }
   ],
   "source": [
    "!python top100_words_simple_text.py /home/vagrant/datacourse/mapreduce/miniprojects/part-000*.xml > top100_text_all.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 1579644), ('of', 947437), ('in', 647037), ('and', 619675), ('a', 573372), ('to', 445456), ('is', 405147), ('ref', 370018), ('category', 325594), ('s', 290175), ('1', 234910), ('http', 216872), ('it', 215571), ('0', 214013), ('was', 212993), ('for', 209574), ('2', 197576), ('on', 186247), ('name', 177114), ('br', 166155)]\n"
     ]
    }
   ],
   "source": [
    "def question2():\n",
    "    with open('top100_text_all.txt') as f:\n",
    "        content = f.readlines()\n",
    "    res1=[]\n",
    "\n",
    "    content = [x.strip() for x in content] \n",
    "    for line in content:\n",
    "        l = line.split()\n",
    "        res1.append((l[0].strip('\"'),int(l[1])))\n",
    "    return res1\n",
    "\n",
    "result2 = question2()\n",
    "print result2[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score:  1.0\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "def top100_words_simple_text():\n",
    "    return [(\"the\", 1577579)] * 100\n",
    "\n",
    "grader.score(question_name='mr__top100_words_simple_text', func=question2)#top100_words_simple_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Question 3: top100_words_simple_no_metadata\n",
    "\n",
    "Finally, notice that 'www' and 'http' make it into the list of top 100 words in the previous problem.  These are also not common English words either!  These are clearly from the url in hyperlinks.  Looking at the format of [Wikipedia links](http://en.wikipedia.org/wiki/Help:Wiki_markup#Links_and_URLs) and [citations](http://en.wikipedia.org/wiki/Help:Wiki_markup#References_and_citing_sources), you'll notice that they tend to appear within single and double brackets and curly braces.\n",
    "\n",
    "**Hint**:\n",
    "You can either write a simple parser to eliminate the urls within brackets, angle braces, and curly braces or you can use a package like the colorfully-named [mwparserfromhell](https://github.com/earwig/mwparserfromhell/), which has been provisioned on `mrjob` and supports the convenient helper function `strip_code()` (which is used by the reference solution).\n",
    "\n",
    "**Checkpoint:**\n",
    "- Total unique words: 618,410"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import mwparserfromhell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{{Infobox islands\n",
      "| name                = Wake Island\n",
      "| image name          = Wake Island map.png\n",
      "| image caption       = Map of Wake Island\n",
      "| image size          = 250px\n",
      "| location            = North Pacific\n",
      "| coordinates         = {{Coord|19|18|N|166|38|E|type:isle_region:UM-79|display=inline,title}}\n",
      "| total islands       = 3\n",
      "| area sqmi           = 2.85\n",
      "| coastline mi        = 12.0\n",
      "| coastline footnotes = <ref>Coastline for Wake Islet: {{convert|12.0|mi|abbr=on}}; Coastline for Wake Atoll: {{\n"
     ]
    }
   ],
   "source": [
    "print content[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "clean_content = mwparserfromhell.parse(content).strip_code()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thumb|upright=1.6|Aerial overview of the atoll\n",
      "Wake Island is an atoll (a type of island) in the Pacific Ocean, near Hawaii. It is controlled by the United States Army and United States Air Force. It is a territory of the United States, part of the United States Minor Outlying Islands.\n",
      "\n",
      " Geography \n",
      "Wake is located to the west of the International Date Line and sits in the Wake Island Time Zone, one day ahead of the 50 U.S. states.\n",
      "\n",
      "Referring to the atoll as an island is the result of a pre-World\n"
     ]
    }
   ],
   "source": [
    "print clean_content[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /home/vagrant/.mrjob.conf\n",
      "Creating temp directory /tmp/top100_words_no_meta.vagrant.20170705.225556.673397\n",
      "Running step 1 of 3...\n",
      "Running step 2 of 3...\n",
      "Running step 3 of 3...\n",
      "Streaming final output from /tmp/top100_words_no_meta.vagrant.20170705.225556.673397/output...\n",
      "Removing temp directory /tmp/top100_words_no_meta.vagrant.20170705.225556.673397...\n"
     ]
    }
   ],
   "source": [
    "!python top100_words_no_meta.py /home/vagrant/datacourse/mapreduce/miniprojects/part-000*.xml > top100_no_meta.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 1431080), ('of', 747461), ('in', 586417), ('and', 547715), ('a', 517220), ('to', 417351), ('is', 391115), ('was', 209489), ('it', 206676), ('for', 185216), ('on', 163112), ('0', 157316), ('that', 154481), ('s', 152092), ('as', 148823), ('align', 141509), ('by', 132277), ('are', 129047), ('1', 126897), ('from', 126530)]\n"
     ]
    }
   ],
   "source": [
    "def question3():\n",
    "    with open('top100_no_meta.txt') as f:\n",
    "        content = f.readlines()\n",
    "    res1=[]\n",
    "\n",
    "    content = [x.strip() for x in content] \n",
    "    for line in content:\n",
    "        l = line.split()\n",
    "        res1.append((l[0].strip('\"'),int(l[1])))\n",
    "    return res1\n",
    "\n",
    "result3 = question3()\n",
    "print result3[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score:  1.0\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "def top100_words_simple_no_metadata():\n",
    "    return [(\"the\", 1427342)] * 100\n",
    "\n",
    "grader.score(question_name='mr__top100_words_simple_no_metadata', func=question3)#top100_words_simple_no_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Question 4: link_stats_simple\n",
    "Let's look at some summary statistics on the number of unique links on a page to other Wikipedia articles.  Return the number of articles (count), average number of links, standard deviation, and the 25%, median, and 75% quantiles.\n",
    "\n",
    "1. Notice that the library `mwparserfromhell` supports the method `filter_wikilinks()`.\n",
    "2. You will need to compute these statistics in a way that requires O(1) memory.  You should be able to compute the first few (i.e. non-quantile) statistics exactly by looking at the first few moments of a distribution. The quantile quantities can be accurately estimated by using reservoir sampling with a large reservoir.\n",
    "3. If there are multiple links to the article have it only count for 1.  This keeps our results from becoming too skewed.\n",
    "4. Don't forget that some (a surprisingly large number of) links have unicode! Make sure you treat them correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'[[United States Air Force]]', u'[[File:Wake Island air.JPG|thumb|upright=1.6|Aerial overview of the atoll]]', u'[[atoll]]', u'[[island]]', u'[[Pacific Ocean]]', u'[[Hawaii]]', u'[[United States Army]]', u'[[United States Air Force]]', u'[[territory]]', u'[[United States]]', u'[[United States Minor Outlying Islands]]', u'[[International date line|International Date Line]]', u'[[Wake Island Time Zone]]', u'[[U.S. state|50 U.S. states]]', u'[[World War II]]', u'[[United States Navy]]', u'[[Japanese]]', u'[[Category:Island insular areas of the United States]]', u'[[Category:Micronesian islands]]', u'[[Category:Atolls]]']\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "import mwparserfromhell\n",
    "content_links = mwparserfromhell.parse(content).filter_wikilinks()\n",
    "print content_links\n",
    "print len(content_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'[[United States Air Force]]', u'[[File:Wake Island air.JPG|thumb|upright=1.6|Aerial overview of the atoll]]', u'[[atoll]]', u'[[island]]', u'[[Pacific Ocean]]', u'[[Hawaii]]', u'[[United States Army]]', u'[[United States Air Force]]', u'[[territory]]', u'[[United States]]', u'[[United States Minor Outlying Islands]]', u'[[International date line|International Date Line]]', u'[[Wake Island Time Zone]]', u'[[U.S. state|50 U.S. states]]', u'[[World War II]]', u'[[United States Navy]]', u'[[Japanese]]', u'[[Category:Island insular areas of the United States]]', u'[[Category:Micronesian islands]]', u'[[Category:Atolls]]']\n",
      "['[[United States Air Force]]', '[[File:Wake Island air.JPG|thumb|upright=1.6|Aerial overview of the atoll]]', '[[atoll]]', '[[island]]', '[[Pacific Ocean]]', '[[Hawaii]]', '[[United States Army]]', '[[United States Air Force]]', '[[territory]]', '[[United States]]', '[[United States Minor Outlying Islands]]', '[[International date line|International Date Line]]', '[[Wake Island Time Zone]]', '[[U.S. state|50 U.S. states]]', '[[World War II]]', '[[United States Navy]]', '[[Japanese]]', '[[Category:Island insular areas of the United States]]', '[[Category:Micronesian islands]]', '[[Category:Atolls]]']\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "links = mwparserfromhell.parse(content).filter_wikilinks()\n",
    "print links\n",
    "\n",
    "links = [l.encode('utf8') for l in links]\n",
    "print links\n",
    "print len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# file: link_stat.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "from lxml import etree\n",
    "import mwparserfromhell\n",
    "from heapq import *\n",
    "import random\n",
    "import math\n",
    "\n",
    "class MRMostUsedWord(MRJob):\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init = self.mapper_init_get_page,\n",
    "                   mapper = self.mapper_get_page), \n",
    "            MRStep(mapper = self.mapper_get_links,\n",
    "                   reducer_init = self.reducer_init_count_links,\n",
    "                   reducer = self.reducer_count_links)\n",
    "        ]\n",
    "\n",
    "    def mapper_init_get_page(self):\n",
    "        self.page_single = []\n",
    "        self.page_status = 0\n",
    "        \n",
    "    def mapper_get_page(self, _, line):\n",
    "        if '<page>' in line:\n",
    "            self.page_single = []\n",
    "            self.page_status = 1\n",
    "            \n",
    "        if self.page_status == 1:\n",
    "            self.page_single.append(line)\n",
    "            \n",
    "        if '</page>' in line:  \n",
    "            if self.page_status == 1:\n",
    "                page = ''.join(self.page_single)\n",
    "                root = etree.XML(page)\n",
    "                content = root.xpath(\"//revision\")[-1].xpath(\".//text\")[0].text\n",
    "                self.page_status = 0\n",
    "                if content:\n",
    "                    content = mwparserfromhell.parse(content).strip_code()\n",
    "                    yield None, content\n",
    "            else:\n",
    "                self.page_status = 0\n",
    "                self.page_single = []\n",
    "            \n",
    "    def mapper_get_links(self, _, line):\n",
    "        links = mwparserfromhell.parse(line).filter_wikilinks()\n",
    "        links = [l.encode('utf8') for l in links]\n",
    "        yield None, len(links)\n",
    "        \n",
    "        \n",
    "    def reducer_init_count_links(self):\n",
    "        self.page_count = 0\n",
    "        self.sum_link = 0\n",
    "        self.sum_linksq = 0\n",
    "        self.linklist = []\n",
    "        \n",
    "    def reducer_count_links(self, _, counts):\n",
    "        for c in counts:\n",
    "            self.page_count += 1\n",
    "            self.sum_link += c\n",
    "            self.sum_linksq += c*c\n",
    "            rand = random.random()\n",
    "            if rand > 0.9:\n",
    "                heappush(self.linklist, c)\n",
    "                #self.linklist.append(c)\n",
    "                \n",
    "        print '\\\"count\\\" , ', self.page_count\n",
    "        avg = float(self.sum_link) / self.page_count\n",
    "        print '\\\"mean\\\" , ', avg\n",
    "        avgsq = float(self.sum_linksq) / self.page_count\n",
    "        std = math.sqrt(avgsq - avg*avg)\n",
    "        print '\\\"stdev\\\" , ', std\n",
    "        #li = self.linklist.sort()\n",
    "        length = len(self.linklist)\n",
    "        li = [ heappop(self.linklist) for i in range(length) ]\n",
    "        l = float(length)\n",
    "        a = int(l/4.0)\n",
    "        b = int(l/2.0)\n",
    "        c = int(l/4.0*3)\n",
    "        print '\\\"25%\\\" , ', li[a]\n",
    "        print '\\\"median\\\" , ', li[b]\n",
    "        print '\\\"75%\\\" , ', li[c]\n",
    "        print 'len(linklist):', length, a,b,c\n",
    "        \n",
    "        yield 0, 0\n",
    "\n",
    "    #def reducer_final_count_links(self): \n",
    "    #you may use reducer_final to print result involving self.XXX in reducer_init\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "#    MRMostUsedWord.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /home/vagrant/.mrjob.conf\n",
      "Creating temp directory /tmp/link_stat.vagrant.20170706.192040.163474\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "\"count\" ,  188408\n",
      "\"mean\" ,  15.6404080506\n",
      "\"stdev\" ,  50.4433023163\n",
      "\"25%\" ,  1\n",
      "\"median\" ,  6\n",
      "\"75%\" ,  17\n",
      "len(linklist): 18826 4706 9413 14119\n",
      "Streaming final output from /tmp/link_stat.vagrant.20170706.192040.163474/output...\n",
      "0\t0\n",
      "Removing temp directory /tmp/link_stat.vagrant.20170706.192040.163474...\n"
     ]
    }
   ],
   "source": [
    "!python link_stat.py /home/vagrant/datacourse/mapreduce/miniprojects/part-000*.xml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /home/vagrant/.mrjob.conf\n",
      "Creating temp directory /tmp/linkstat.vagrant.20170706.044731.905371\n",
      "Running step 1 of 1...\n",
      "linkstat.py:30: FutureWarning: The behavior of this method will change in future versions. Use specific 'len(elem)' or 'elem is not None' test instead.\n",
      "  texts = root and root.xpath('//text')\n",
      "Streaming final output from /tmp/linkstat.vagrant.20170706.044731.905371/output...\n",
      "Removing temp directory /tmp/linkstat.vagrant.20170706.044731.905371...\n"
     ]
    }
   ],
   "source": [
    "!python linkstat.py /home/vagrant/datacourse/mapreduce/miniprojects/part-000*.xml > linkstat_00.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score:  1.0\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "def link_stats_simple():\n",
    "    return [\n",
    "        (\"count\", 188408),\n",
    "        (\"mean\", 15.6404080506),\n",
    "        (\"stdev\", 50.4433023163),\n",
    "        (\"25%\", 1),\n",
    "        (\"median\", 6),\n",
    "        (\"75%\", 17),\n",
    "    ]\n",
    "\n",
    "grader.score(question_name='mr__link_stats_simple', func=link_stats_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*Copyright &copy; 2016 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
